{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3i1KfYGFRRWP"
      },
      "source": [
        "## Artificial Neural Networks and Deep Learning  \n",
        "##Assignment 3.3 - Self-attention and Transformers\n",
        "\n",
        "Prof. Dr. Ir. Johan A. K. Suykens     \n",
        "\n",
        "In this file, we first understand the self-attention mechanism by implementing it both with ``NumPy`` and ``PyTorch``.\n",
        "Then, we implement a 6-layer Vision Transformer (ViT) and train it on the MNIST dataset.\n",
        "\n",
        "All training will be conducted on a single T4 GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlTJbgaaRTct",
        "outputId": "4a3f8c4a-9478-4642-d065-fc2070558c2d"
      },
      "outputs": [],
      "source": [
        "# Please first load your google drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Qu6w5GLkRezN"
      },
      "outputs": [],
      "source": [
        "# Please go to Edit > Notebook settings > Hardware accelerator > choose \"T4 GPU\"\n",
        "# Now check if you have loaded the GPU successfully\n",
        "# !nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-sUz1A9SzVH"
      },
      "source": [
        "# Self-attention Mechanism\n",
        "Self-attention is the core mechanism in Transformer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6ol1XZtiPpk"
      },
      "source": [
        "## Self-attention with NumPy\n",
        "To have a better understanding of it, we first manually implement self-attention mechanism with ``numpy``. You can check the dimension of each variable during the matrix computation.\n",
        "\n",
        "Feel free to change the dimensions of each variable and see how the output dimension will change accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AgWIgp51RgC3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The attention outputs are\n",
            " [[ 0.11117146 -0.62926768 -0.85849651 ... -2.53897437 -1.69939913\n",
            "  -0.14339829]\n",
            " [ 0.11117146 -0.62926768 -0.85849651 ... -2.53897437 -1.69939913\n",
            "  -0.14339829]\n",
            " [ 0.11117146 -0.62926768 -0.85849651 ... -2.53897437 -1.69939913\n",
            "  -0.14339829]\n",
            " ...\n",
            " [ 0.11117146 -0.62926768 -0.85849651 ... -2.53897437 -1.69939913\n",
            "  -0.14339829]\n",
            " [ 0.11117146 -0.62926768 -0.85849651 ... -2.53897437 -1.69939913\n",
            "  -0.14339829]\n",
            " [ 0.11117146 -0.62926768 -0.85849651 ... -2.53897437 -1.69939913\n",
            "  -0.14339829]]\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from numpy.random import randn\n",
        "\n",
        "# I. Define the input data X\n",
        "# X consists out of 32 samples, each sample has dimensionality 256\n",
        "n = 32\n",
        "d = 256\n",
        "X = randn(n, d) # (32, 256)\n",
        "\n",
        "# II. Generate the projection weights\n",
        "Wq = randn(d, d) #(256, 256)\n",
        "Wk = randn(d, d)\n",
        "Wv = randn(d, d)\n",
        "\n",
        "# III. Project X to find its query, keys and values vectors\n",
        "Q = np.dot(X, Wq) # (32, 256)\n",
        "K = np.dot(X, Wk)\n",
        "V = np.dot(X, Wv)\n",
        "\n",
        "# IV. Compute the self-attention score, denoted by A\n",
        "# A = softmax(QK^T / \\sqrt{d})\n",
        "# Define the softmax function\n",
        "def softmax(z):\n",
        "    z = np.clip(z, 100, -100) # clip in case softmax explodes\n",
        "    tmp = np.exp(z)\n",
        "    res = np.exp(z) / np.sum(tmp, axis=1)\n",
        "    return res\n",
        "\n",
        "A = softmax(np.dot(Q, K.transpose())/math.sqrt(d)) #(32, 32)\n",
        "\n",
        "# V. Compute the self-attention output\n",
        "# outputs = A * V\n",
        "outputs = np.dot(A, V) #(32, 256)\n",
        "\n",
        "print(\"The attention outputs are\\n {}\".format(outputs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iozM1k4khO0B"
      },
      "source": [
        "## Self-attention with PyTorch\n",
        "Now, we implement self-attention with ``PyTorch``, which is commonly used when building Transformers.\n",
        "\n",
        "Feel free to change the dimensions of each variable and see how the output dimension will change accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qng07v8xdaPj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x.shape:torch.Size([32, 20, 128]) \n",
            " Q.shape:torch.Size([32, 20, 64]) \n",
            " K.shape:torch.Size([32, 20, 64]) \n",
            " V.shape:torch.Size([32, 20, 32])\n",
            "attention matrix:  torch.Size([32, 20, 20])\n",
            "attention outputs:  torch.Size([32, 20, 32])\n",
            "tensor([[[-0.2571, -0.0328,  0.1054,  ..., -0.2961,  0.2488,  0.0892],\n",
            "         [-0.1916,  0.0009,  0.1326,  ..., -0.3157,  0.2430,  0.0560],\n",
            "         [-0.1671, -0.0994,  0.2084,  ..., -0.3470,  0.2002,  0.1431],\n",
            "         ...,\n",
            "         [-0.1988,  0.0154,  0.0519,  ..., -0.2714,  0.1825, -0.0136],\n",
            "         [-0.1966,  0.0350,  0.0949,  ..., -0.3174,  0.2123,  0.0848],\n",
            "         [-0.2472, -0.0279,  0.1560,  ..., -0.3049,  0.2185,  0.0559]],\n",
            "\n",
            "        [[-0.0409,  0.1236, -0.0677,  ..., -0.1354, -0.1250, -0.0538],\n",
            "         [ 0.0176,  0.3180, -0.0096,  ..., -0.0826, -0.1564,  0.0498],\n",
            "         [ 0.0083,  0.1859, -0.0263,  ..., -0.0763, -0.1049,  0.0308],\n",
            "         ...,\n",
            "         [-0.0237,  0.1442, -0.0643,  ..., -0.1065, -0.1581, -0.0142],\n",
            "         [-0.0210,  0.2194, -0.0355,  ..., -0.0645, -0.1718,  0.0406],\n",
            "         [-0.0209,  0.2382, -0.1097,  ..., -0.1139, -0.1788,  0.0202]],\n",
            "\n",
            "        [[ 0.1007,  0.1640, -0.0251,  ..., -0.1757,  0.0101,  0.0443],\n",
            "         [ 0.0440,  0.0396, -0.0062,  ..., -0.2154,  0.0333,  0.0628],\n",
            "         [ 0.1200,  0.1234,  0.0491,  ..., -0.2164,  0.0775, -0.0404],\n",
            "         ...,\n",
            "         [ 0.1002,  0.0070, -0.0756,  ..., -0.1616, -0.0028,  0.1046],\n",
            "         [ 0.0561, -0.0663, -0.0115,  ..., -0.1875,  0.0470, -0.0211],\n",
            "         [ 0.0389,  0.0550, -0.0671,  ..., -0.2022,  0.0213,  0.0414]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0289, -0.1263,  0.2633,  ...,  0.2938, -0.0343,  0.3006],\n",
            "         [ 0.0279, -0.1141,  0.2760,  ...,  0.2822, -0.0786,  0.2934],\n",
            "         [-0.0084, -0.1108,  0.2427,  ...,  0.3229,  0.0247,  0.2007],\n",
            "         ...,\n",
            "         [-0.0359, -0.0929,  0.2671,  ...,  0.2240, -0.0157,  0.2000],\n",
            "         [ 0.0076, -0.1135,  0.2643,  ...,  0.2924, -0.0031,  0.2289],\n",
            "         [ 0.0084, -0.0491,  0.2758,  ...,  0.1687, -0.0216,  0.1904]],\n",
            "\n",
            "        [[ 0.2185,  0.1913, -0.0656,  ..., -0.0596, -0.0292, -0.0804],\n",
            "         [ 0.1181,  0.2291, -0.0023,  ..., -0.0273,  0.0268, -0.0400],\n",
            "         [ 0.1198,  0.2692,  0.0305,  ...,  0.0208,  0.0784, -0.0686],\n",
            "         ...,\n",
            "         [ 0.1679,  0.1659, -0.0476,  ..., -0.0631,  0.0308, -0.0113],\n",
            "         [ 0.2078,  0.1421, -0.0485,  ..., -0.0453, -0.1221, -0.0659],\n",
            "         [ 0.0930,  0.1658,  0.0219,  ..., -0.0925,  0.0252, -0.0241]],\n",
            "\n",
            "        [[-0.0167, -0.2442, -0.0338,  ..., -0.2009, -0.0531, -0.1528],\n",
            "         [ 0.0719, -0.2776, -0.0503,  ..., -0.1209, -0.1044, -0.1010],\n",
            "         [-0.0165, -0.1734, -0.0922,  ..., -0.2442, -0.0881, -0.0689],\n",
            "         ...,\n",
            "         [ 0.0378, -0.2147, -0.0439,  ..., -0.2257, -0.0520, -0.1346],\n",
            "         [ 0.0477, -0.2069,  0.0314,  ..., -0.1404, -0.1305, -0.0811],\n",
            "         [ 0.0480, -0.2153,  0.0744,  ..., -0.0984, -0.0790, -0.0850]]],\n",
            "       grad_fn=<BmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, dim_input, dim_q, dim_v):\n",
        "        '''\n",
        "        dim_input: the dimension of each sample\n",
        "        dim_q: dimension of Q matrix, should be equal to dim_k\n",
        "        dim_v: dimension of V matrix, also the  dimension of the attention output\n",
        "        '''\n",
        "        super(SelfAttention, self).__init__()\n",
        "\n",
        "        self.dim_input = dim_input\n",
        "        self.dim_q = dim_q\n",
        "        self.dim_k = dim_q\n",
        "        self.dim_v = dim_v\n",
        "\n",
        "        # Define the linear projection\n",
        "        self.linear_q = nn.Linear(self.dim_input, self.dim_q, bias=False)\n",
        "        self.linear_k = nn.Linear(self.dim_input, self.dim_k, bias=False)\n",
        "        self.linear_v = nn.Linear(self.dim_input, self.dim_v, bias=False)\n",
        "        self._norm_fact = 1 / math.sqrt(self.dim_k)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, n, dim_q = x.shape\n",
        "\n",
        "        q = self.linear_q(x) # (batchsize, seq_len, dim_q)\n",
        "        k = self.linear_k(x) # (batchsize, seq_len, dim_k)\n",
        "        v = self.linear_v(x) # (batchsize, seq_len, dim_v)\n",
        "        print(f'x.shape:{x.shape} \\n Q.shape:{q.shape} \\n K.shape:{k.shape} \\n V.shape:{v.shape}')\n",
        "\n",
        "        dist = torch.bmm(q, k.transpose(1,2)) * self._norm_fact\n",
        "        dist = torch.softmax(dist, dim=-1)\n",
        "        print('attention matrix: ', dist.shape)\n",
        "\n",
        "        outputs = torch.bmm(dist, v)\n",
        "        print('attention outputs: ', outputs.shape)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "batch_size = 32 # number of samples in a batch\n",
        "dim_input = 128 # dimension of each item in the sample sequence\n",
        "seq_len = 20 # sequence length for each sample\n",
        "x = torch.randn(batch_size, seq_len, dim_input)\n",
        "self_attention = SelfAttention(dim_input, dim_q = 64, dim_v = 32)\n",
        "\n",
        "attention = self_attention(x)\n",
        "\n",
        "print(attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZaAFL8MS2Ng"
      },
      "source": [
        "# Transformers\n",
        "In this section, we implement a 6-layer Vision Transformer (ViT) and trained it on the MNIST dataset.\n",
        "We consider the classification tasks.\n",
        "First, we load the MNIST dataset as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rZ-eIaeZjWjL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torchvision import datasets, utils\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "def get_mnist_loader(batch_size=100, shuffle=True):\n",
        "    \"\"\"\n",
        "\n",
        "    :return: train_loader, test_loader\n",
        "    \"\"\"\n",
        "    train_dataset = MNIST(root='../data',\n",
        "                          train=True,\n",
        "                          transform=torchvision.transforms.ToTensor(),\n",
        "                          download=True)\n",
        "    test_dataset = MNIST(root='../data',\n",
        "                         train=False,\n",
        "                         transform=torchvision.transforms.ToTensor(),\n",
        "                         download=True)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=shuffle)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                              batch_size=batch_size,\n",
        "                                              shuffle=False)\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-C06IoPIjePg"
      },
      "outputs": [],
      "source": [
        "# This package is needed to build the transformer\n",
        "# !pip install einops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx9eZrMpmA2z"
      },
      "source": [
        "## Build ViT from scratch\n",
        "Recall that each Transformer block include 2 modules: the self-attention module, the feedforward module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Vr6d7IWfjpxY"
      },
      "outputs": [],
      "source": [
        "from einops import rearrange\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads=8):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.scale = dim ** -0.5\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
        "        self.to_out = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "        b, n, _, h = *x.shape, self.heads\n",
        "        qkv = self.to_qkv(x)\n",
        "        q, k, v = rearrange(qkv, 'b n (qkv h d) -> qkv b h n d', qkv=3, h=h)\n",
        "\n",
        "        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = F.pad(mask.flatten(1), (1, 0), value = True)\n",
        "            assert mask.shape[-1] == dots.shape[-1], 'mask has incorrect dimensions'\n",
        "            mask = mask[:, None, :] * mask[:, :, None]\n",
        "            dots.masked_fill_(~mask, float('-inf'))\n",
        "            del mask\n",
        "\n",
        "        attn = dots.softmax(dim=-1)\n",
        "\n",
        "        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out =  self.to_out(out)\n",
        "        return out\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                Residual(PreNorm(dim, Attention(dim, heads = heads))),\n",
        "                Residual(PreNorm(dim, FeedForward(dim, mlp_dim)))\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x, mask=mask)\n",
        "            x = ff(x)\n",
        "        return x\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels=3):\n",
        "        super().__init__()\n",
        "        assert image_size % patch_size == 0, 'image dimensions must be divisible by the patch size'\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        patch_dim = channels * patch_size ** 2\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.transformer = Transformer(dim, depth, heads, mlp_dim)\n",
        "\n",
        "        self.to_cls_token = nn.Identity()\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_dim),\n",
        "            nn.GELU(), # Gaussian Error Linear Units is another type of activation function\n",
        "            nn.Linear(mlp_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img, mask=None):\n",
        "        p = self.patch_size\n",
        "\n",
        "        x = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
        "        x = self.patch_to_embedding(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(img.shape[0], -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.pos_embedding\n",
        "        x = self.transformer(x, mask)\n",
        "\n",
        "        x = self.to_cls_token(x[:, 0])\n",
        "        return self.mlp_head(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTawNC64mhBO"
      },
      "source": [
        "## Training and test function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rKJ4tjCjjycH"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def train_epoch(model, optimizer, data_loader, loss_history):\n",
        "    total_samples = len(data_loader.dataset)\n",
        "    model.train()\n",
        "\n",
        "    for i, (data, target) in enumerate(data_loader):\n",
        "        # data = data.cuda()\n",
        "        # target = target.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        output = F.log_softmax(model(data), dim=1)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print('[' +  '{:5}'.format(i * len(data)) + '/' + '{:5}'.format(total_samples) +\n",
        "                  ' (' + '{:3.0f}'.format(100 * i / len(data_loader)) + '%)]  Loss: ' +\n",
        "                  '{:6.4f}'.format(loss.item()))\n",
        "            loss_history.append(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "vph2CrNxj6ZZ"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data_loader, loss_history):\n",
        "    model.eval()\n",
        "\n",
        "    total_samples = len(data_loader.dataset)\n",
        "    correct_samples = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    # We do not need to remember the gradients when testing\n",
        "    # This will help reduce memory\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            # data = data.cuda()\n",
        "            # target = target.cuda()\n",
        "            output = F.log_softmax(model(data), dim=1)\n",
        "            loss = F.nll_loss(output, target, reduction='sum')\n",
        "            _, pred = torch.max(output, dim=1)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            correct_samples += pred.eq(target).sum()\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    loss_history.append(avg_loss)\n",
        "    print('\\nAverage test loss: ' + '{:.4f}'.format(avg_loss) +\n",
        "          '  Accuracy:' + '{:5}'.format(correct_samples) + '/' +\n",
        "          '{:5}'.format(total_samples) + ' (' +\n",
        "          '{:4.2f}'.format(100.0 * correct_samples / total_samples) + '%)\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRYys50km0-E"
      },
      "source": [
        "## Let's start training!\n",
        "Here, you can change the ViT structure by changing the hyper-parametrs inside ``ViT`` function.\n",
        "The default settings are with 6 layers, 8 heads for the multi-head attention mechanism and embedding dimension of 64.\n",
        "You can also increase the number of epochs to obtain better results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "rVLJLLDuj7yQ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# You can change the architecture here\n",
        "model = ViT(image_size=28, patch_size=7, num_classes=10, channels=1,\n",
        "            dim=64, depth=6, heads=8, mlp_dim=128)\n",
        "# model = model.cuda()\n",
        "# We also print the network architecture\n",
        "model\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "train_loss_history, test_loss_history = [], []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Vlt3tk-MkDB9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.4034\n",
            "[12800/60000 ( 21%)]  Loss: 1.0005\n",
            "[25600/60000 ( 43%)]  Loss: 0.6079\n",
            "[38400/60000 ( 64%)]  Loss: 0.3893\n",
            "[51200/60000 ( 85%)]  Loss: 0.1894\n",
            "\n",
            "Average test loss: 0.2200  Accuracy: 9272/10000 (92.72%)\n",
            "\n",
            "Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.2296\n",
            "[12800/60000 ( 21%)]  Loss: 0.1666\n",
            "[25600/60000 ( 43%)]  Loss: 0.1298\n",
            "[38400/60000 ( 64%)]  Loss: 0.1497\n",
            "[51200/60000 ( 85%)]  Loss: 0.1563\n",
            "\n",
            "Average test loss: 0.1525  Accuracy: 9521/10000 (95.21%)\n",
            "\n",
            "Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.0492\n",
            "[12800/60000 ( 21%)]  Loss: 0.1015\n",
            "[25600/60000 ( 43%)]  Loss: 0.0958\n",
            "[38400/60000 ( 64%)]  Loss: 0.1118\n",
            "[51200/60000 ( 85%)]  Loss: 0.1123\n",
            "\n",
            "Average test loss: 0.1087  Accuracy: 9663/10000 (96.63%)\n",
            "\n",
            "Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.0789\n",
            "[12800/60000 ( 21%)]  Loss: 0.1162\n",
            "[25600/60000 ( 43%)]  Loss: 0.1087\n",
            "[38400/60000 ( 64%)]  Loss: 0.1008\n",
            "[51200/60000 ( 85%)]  Loss: 0.1376\n",
            "\n",
            "Average test loss: 0.0934  Accuracy: 9692/10000 (96.92%)\n",
            "\n",
            "Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.0933\n",
            "[12800/60000 ( 21%)]  Loss: 0.0086\n",
            "[25600/60000 ( 43%)]  Loss: 0.0400\n",
            "[38400/60000 ( 64%)]  Loss: 0.0258\n",
            "[51200/60000 ( 85%)]  Loss: 0.1472\n",
            "\n",
            "Average test loss: 0.0784  Accuracy: 9737/10000 (97.37%)\n",
            "\n",
            "Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.0612\n",
            "[12800/60000 ( 21%)]  Loss: 0.0935\n",
            "[25600/60000 ( 43%)]  Loss: 0.0769\n",
            "[38400/60000 ( 64%)]  Loss: 0.0309\n",
            "[51200/60000 ( 85%)]  Loss: 0.0742\n",
            "\n",
            "Average test loss: 0.0848  Accuracy: 9731/10000 (97.31%)\n",
            "\n",
            "Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.0489\n",
            "[12800/60000 ( 21%)]  Loss: 0.1009\n",
            "[25600/60000 ( 43%)]  Loss: 0.0256\n",
            "[38400/60000 ( 64%)]  Loss: 0.0481\n",
            "[51200/60000 ( 85%)]  Loss: 0.0535\n",
            "\n",
            "Average test loss: 0.0607  Accuracy: 9810/10000 (98.10%)\n",
            "\n",
            "Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.0571\n",
            "[12800/60000 ( 21%)]  Loss: 0.0322\n",
            "[25600/60000 ( 43%)]  Loss: 0.0466\n",
            "[38400/60000 ( 64%)]  Loss: 0.0756\n",
            "[51200/60000 ( 85%)]  Loss: 0.1203\n",
            "\n",
            "Average test loss: 0.0680  Accuracy: 9786/10000 (97.86%)\n",
            "\n",
            "Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0391\n",
            "[12800/60000 ( 21%)]  Loss: 0.0081\n",
            "[25600/60000 ( 43%)]  Loss: 0.0738\n",
            "[38400/60000 ( 64%)]  Loss: 0.0294\n",
            "[51200/60000 ( 85%)]  Loss: 0.0901\n",
            "\n",
            "Average test loss: 0.0643  Accuracy: 9806/10000 (98.06%)\n",
            "\n",
            "Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0159\n",
            "[12800/60000 ( 21%)]  Loss: 0.0080\n",
            "[25600/60000 ( 43%)]  Loss: 0.0235\n",
            "[38400/60000 ( 64%)]  Loss: 0.0416\n",
            "[51200/60000 ( 85%)]  Loss: 0.0258\n",
            "\n",
            "Average test loss: 0.0605  Accuracy: 9824/10000 (98.24%)\n",
            "\n",
            "Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.0063\n",
            "[12800/60000 ( 21%)]  Loss: 0.0013\n",
            "[25600/60000 ( 43%)]  Loss: 0.0102\n",
            "[38400/60000 ( 64%)]  Loss: 0.0074\n",
            "[51200/60000 ( 85%)]  Loss: 0.0057\n",
            "\n",
            "Average test loss: 0.0651  Accuracy: 9805/10000 (98.05%)\n",
            "\n",
            "Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0262\n",
            "[12800/60000 ( 21%)]  Loss: 0.0375\n",
            "[25600/60000 ( 43%)]  Loss: 0.0150\n",
            "[38400/60000 ( 64%)]  Loss: 0.0091\n",
            "[51200/60000 ( 85%)]  Loss: 0.0018\n",
            "\n",
            "Average test loss: 0.0656  Accuracy: 9804/10000 (98.04%)\n",
            "\n",
            "Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0062\n",
            "[12800/60000 ( 21%)]  Loss: 0.0081\n",
            "[25600/60000 ( 43%)]  Loss: 0.0239\n",
            "[38400/60000 ( 64%)]  Loss: 0.0202\n",
            "[51200/60000 ( 85%)]  Loss: 0.0132\n",
            "\n",
            "Average test loss: 0.0924  Accuracy: 9750/10000 (97.50%)\n",
            "\n",
            "Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0098\n",
            "[12800/60000 ( 21%)]  Loss: 0.0165\n",
            "[25600/60000 ( 43%)]  Loss: 0.0343\n",
            "[38400/60000 ( 64%)]  Loss: 0.0028\n",
            "[51200/60000 ( 85%)]  Loss: 0.0036\n",
            "\n",
            "Average test loss: 0.0850  Accuracy: 9785/10000 (97.85%)\n",
            "\n",
            "Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0026\n",
            "[12800/60000 ( 21%)]  Loss: 0.0102\n",
            "[25600/60000 ( 43%)]  Loss: 0.0009\n",
            "[38400/60000 ( 64%)]  Loss: 0.0148\n",
            "[51200/60000 ( 85%)]  Loss: 0.0174\n",
            "\n",
            "Average test loss: 0.0674  Accuracy: 9839/10000 (98.39%)\n",
            "\n",
            "Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0189\n",
            "[12800/60000 ( 21%)]  Loss: 0.0052\n",
            "[25600/60000 ( 43%)]  Loss: 0.0044\n",
            "[38400/60000 ( 64%)]  Loss: 0.0156\n",
            "[51200/60000 ( 85%)]  Loss: 0.0071\n",
            "\n",
            "Average test loss: 0.0751  Accuracy: 9814/10000 (98.14%)\n",
            "\n",
            "Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0159\n",
            "[12800/60000 ( 21%)]  Loss: 0.0024\n",
            "[25600/60000 ( 43%)]  Loss: 0.0009\n",
            "[38400/60000 ( 64%)]  Loss: 0.0013\n",
            "[51200/60000 ( 85%)]  Loss: 0.0032\n",
            "\n",
            "Average test loss: 0.0784  Accuracy: 9815/10000 (98.15%)\n",
            "\n",
            "Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0021\n",
            "[12800/60000 ( 21%)]  Loss: 0.0015\n",
            "[25600/60000 ( 43%)]  Loss: 0.0016\n",
            "[38400/60000 ( 64%)]  Loss: 0.0348\n",
            "[51200/60000 ( 85%)]  Loss: 0.0067\n",
            "\n",
            "Average test loss: 0.0850  Accuracy: 9808/10000 (98.08%)\n",
            "\n",
            "Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0285\n",
            "[12800/60000 ( 21%)]  Loss: 0.0012\n",
            "[25600/60000 ( 43%)]  Loss: 0.0019\n",
            "[38400/60000 ( 64%)]  Loss: 0.0011\n",
            "[51200/60000 ( 85%)]  Loss: 0.0032\n",
            "\n",
            "Average test loss: 0.0786  Accuracy: 9831/10000 (98.31%)\n",
            "\n",
            "Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0066\n",
            "[12800/60000 ( 21%)]  Loss: 0.0005\n",
            "[25600/60000 ( 43%)]  Loss: 0.0003\n",
            "[38400/60000 ( 64%)]  Loss: 0.0022\n",
            "[51200/60000 ( 85%)]  Loss: 0.0020\n",
            "\n",
            "Average test loss: 0.0743  Accuracy: 9831/10000 (98.31%)\n",
            "\n",
            "Execution time: 646.72 seconds\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 20\n",
        "\n",
        "train_loader, test_loader = get_mnist_loader(batch_size=128, shuffle=True)\n",
        "\n",
        "# Gradually reduce the learning rate while training\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    print('Epoch:', epoch,'LR:', scheduler.get_last_lr())\n",
        "    train_epoch(model, optimizer, train_loader, train_loss_history)\n",
        "    evaluate(model, test_loader, test_loss_history)\n",
        "    scheduler.step()\n",
        "\n",
        "print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XicoRf8_nUTK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
