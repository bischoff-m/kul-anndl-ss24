{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3i1KfYGFRRWP"
      },
      "source": [
        "# Artificial Neural Networks and Deep Learning  \n",
        "## Assignment 3.3 - Self-attention and Transformers\n",
        "\n",
        "Prof. Dr. Ir. Johan A. K. Suykens     \n",
        "\n",
        "In this file, we first understand the self-attention mechanism by implementing it both with ``NumPy`` and ``PyTorch``.\n",
        "Then, we implement a 6-layer Vision Transformer (ViT) and train it on the MNIST dataset.\n",
        "\n",
        "All training will be conducted on a single T4 GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlTJbgaaRTct",
        "outputId": "4a3f8c4a-9478-4642-d065-fc2070558c2d"
      },
      "outputs": [],
      "source": [
        "# Please first load your google drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Qu6w5GLkRezN"
      },
      "outputs": [],
      "source": [
        "# Please go to Edit > Notebook settings > Hardware accelerator > choose \"T4 GPU\"\n",
        "# Now check if you have loaded the GPU successfully\n",
        "# !nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "from pathlib import Path\n",
        "\n",
        "class Store:\n",
        "\t\"\"\"\n",
        "\tTODO: Add \"default\" parameter to function like a defaultdict\n",
        "\tTODO: Add \"expected keys\" property to iterate over (non-)missing keys\n",
        "\t\t\t- Could also be based on a pydantic model to define expected structure\n",
        "\tTODO: Add method to remove unexpected keys\n",
        "\t\"\"\"\n",
        "\tdef __init__(self, filename):\n",
        "\t\tself.filename = filename\n",
        "\t\tself.data = {}\n",
        "\t\tif Path(filename).exists():\n",
        "\t\t\twith open(filename, 'rb') as f:\n",
        "\t\t\t\tself.data = pickle.load(f)\n",
        "\t\telse:\n",
        "\t\t\twith open(filename, 'wb') as f:\n",
        "\t\t\t\tpickle.dump(self.data, f)\n",
        "\n",
        "\tdef __getitem__(self, key):\n",
        "\t\treturn self.data[key]\n",
        "\t\n",
        "\tdef __setitem__(self, key, value):\n",
        "\t\tself.data[key] = value\n",
        "\t\twith open(self.filename, 'wb') as f:\n",
        "\t\t\tpickle.dump(self.data, f)\n",
        "\n",
        "\tdef __contains__(self, key):\n",
        "\t\treturn key in self.data\n",
        "\t\n",
        "\tdef __delitem__(self, key):\n",
        "\t\tdel self.data[key]\n",
        "\t\twith open(self.filename, 'wb') as f:\n",
        "\t\t\tpickle.dump(self.data, f)\n",
        "\n",
        "\tdef __iter__(self):\n",
        "\t\treturn iter(self.data)\n",
        "\t\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.data)\n",
        "\t\n",
        "\tdef keys(self):\n",
        "\t\treturn self.data.keys()\n",
        "\t\n",
        "\tdef values(self):\n",
        "\t\treturn self.data.values()\n",
        "\t\n",
        "\tdef items(self):\n",
        "\t\treturn self.data.items()\n",
        "\t\n",
        "\tdef clear(self):\n",
        "\t\tself.data.clear()\n",
        "\t\twith open(self.filename, 'wb') as f:\n",
        "\t\t\tpickle.dump(self.data, f)\n",
        "\n",
        "\tdef __str__(self):\n",
        "\t\treturn str(self.data)\n",
        "\t\n",
        "\tdef __repr__(self):\n",
        "\t\treturn repr(self.data)\n",
        "\t\n",
        "\tdef __del__(self):\n",
        "\t\twith open(self.filename, 'wb') as f:\n",
        "\t\t\tpickle.dump(self.data, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-sUz1A9SzVH"
      },
      "source": [
        "# Self-attention Mechanism\n",
        "Self-attention is the core mechanism in Transformer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6ol1XZtiPpk"
      },
      "source": [
        "## Self-attention with NumPy\n",
        "To have a better understanding of it, we first manually implement self-attention mechanism with ``numpy``. You can check the dimension of each variable during the matrix computation.\n",
        "\n",
        "Feel free to change the dimensions of each variable and see how the output dimension will change accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "AgWIgp51RgC3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The attention outputs are\n",
            " [[-3.5279123  -1.96080231 -1.15167238 ... -2.95900417  0.7092281\n",
            "  -2.12301188]\n",
            " [-3.5279123  -1.96080231 -1.15167238 ... -2.95900417  0.7092281\n",
            "  -2.12301188]\n",
            " [-3.5279123  -1.96080231 -1.15167238 ... -2.95900417  0.7092281\n",
            "  -2.12301188]\n",
            " ...\n",
            " [-3.5279123  -1.96080231 -1.15167238 ... -2.95900417  0.7092281\n",
            "  -2.12301188]\n",
            " [-3.5279123  -1.96080231 -1.15167238 ... -2.95900417  0.7092281\n",
            "  -2.12301188]\n",
            " [-3.5279123  -1.96080231 -1.15167238 ... -2.95900417  0.7092281\n",
            "  -2.12301188]]\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from numpy.random import randn\n",
        "from pathlib import Path\n",
        "\n",
        "FIGURE_PATH = Path(\"../report/figures\")\n",
        "\n",
        "# I. Define the input data X\n",
        "# X consists out of 32 samples, each sample has dimensionality 256\n",
        "n = 32\n",
        "d = 256\n",
        "X = randn(n, d) # (32, 256)\n",
        "\n",
        "# II. Generate the projection weights\n",
        "Wq = randn(d, d) #(256, 256)\n",
        "Wk = randn(d, d)\n",
        "Wv = randn(d, d)\n",
        "\n",
        "# III. Project X to find its query, keys and values vectors\n",
        "Q = np.dot(X, Wq) # (32, 256)\n",
        "K = np.dot(X, Wk)\n",
        "V = np.dot(X, Wv)\n",
        "\n",
        "# IV. Compute the self-attention score, denoted by A\n",
        "# A = softmax(QK^T / \\sqrt{d})\n",
        "# Define the softmax function\n",
        "def softmax(z):\n",
        "    z = np.clip(z, 100, -100) # clip in case softmax explodes\n",
        "    tmp = np.exp(z)\n",
        "    res = np.exp(z) / np.sum(tmp, axis=1)\n",
        "    return res\n",
        "\n",
        "A = softmax(np.dot(Q, K.transpose())/math.sqrt(d)) #(32, 32)\n",
        "\n",
        "# V. Compute the self-attention output\n",
        "# outputs = A * V\n",
        "outputs = np.dot(A, V) #(32, 256)\n",
        "\n",
        "print(\"The attention outputs are\\n {}\".format(outputs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iozM1k4khO0B"
      },
      "source": [
        "## Self-attention with PyTorch\n",
        "Now, we implement self-attention with ``PyTorch``, which is commonly used when building Transformers.\n",
        "\n",
        "Feel free to change the dimensions of each variable and see how the output dimension will change accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "qng07v8xdaPj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x.shape:torch.Size([32, 20, 128]) \n",
            " Q.shape:torch.Size([32, 20, 64]) \n",
            " K.shape:torch.Size([32, 20, 64]) \n",
            " V.shape:torch.Size([32, 20, 32])\n",
            "attention matrix:  torch.Size([32, 20, 20])\n",
            "attention outputs:  torch.Size([32, 20, 32])\n",
            "tensor([[[ 0.0925, -0.1243,  0.0035,  ..., -0.1726, -0.0626,  0.2192],\n",
            "         [ 0.0393, -0.0528,  0.0338,  ..., -0.1209,  0.0259,  0.3022],\n",
            "         [ 0.0438, -0.1083,  0.0151,  ..., -0.2410, -0.0749,  0.3585],\n",
            "         ...,\n",
            "         [ 0.0992, -0.0442, -0.0219,  ..., -0.2382, -0.2141,  0.3080],\n",
            "         [ 0.1225, -0.1314, -0.0034,  ..., -0.2117, -0.0582,  0.0981],\n",
            "         [ 0.0430, -0.1030,  0.0585,  ..., -0.3413, -0.1247,  0.2808]],\n",
            "\n",
            "        [[-0.1428,  0.2679,  0.1408,  ...,  0.0352, -0.0580,  0.0792],\n",
            "         [-0.0842,  0.2698,  0.1667,  ...,  0.0574,  0.0110,  0.1966],\n",
            "         [-0.1172,  0.0103,  0.1846,  ...,  0.0377, -0.0461,  0.1214],\n",
            "         ...,\n",
            "         [-0.1607,  0.2252,  0.0799,  ...,  0.0043, -0.0240,  0.2271],\n",
            "         [-0.0765,  0.2453,  0.0232,  ...,  0.0793, -0.0578,  0.0837],\n",
            "         [-0.1362,  0.2412,  0.0709,  ...,  0.0843, -0.0703,  0.1914]],\n",
            "\n",
            "        [[ 0.0119, -0.0959, -0.1623,  ...,  0.1411,  0.0627, -0.1494],\n",
            "         [-0.0335, -0.1099, -0.1314,  ...,  0.1033, -0.0311, -0.1185],\n",
            "         [-0.0526, -0.0804, -0.2094,  ...,  0.1137, -0.0138, -0.1277],\n",
            "         ...,\n",
            "         [-0.0149, -0.1062, -0.1808,  ...,  0.1128,  0.0065, -0.0906],\n",
            "         [-0.0862, -0.0851, -0.1394,  ...,  0.1417,  0.0368, -0.1010],\n",
            "         [ 0.0969, -0.0836, -0.2034,  ...,  0.1985, -0.0034, -0.0310]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.1339, -0.2797,  0.0964,  ..., -0.1035, -0.0781, -0.1224],\n",
            "         [ 0.1680, -0.2963,  0.0569,  ..., -0.0157, -0.0313, -0.0203],\n",
            "         [ 0.1443, -0.2329,  0.0051,  ..., -0.0368, -0.1497, -0.0505],\n",
            "         ...,\n",
            "         [ 0.2300, -0.2899, -0.0067,  ...,  0.0405, -0.0549, -0.0982],\n",
            "         [ 0.2083, -0.2602,  0.0097,  ...,  0.0286, -0.1465, -0.1124],\n",
            "         [ 0.1710, -0.2257,  0.0016,  ..., -0.0022, -0.0512, -0.0204]],\n",
            "\n",
            "        [[-0.2098,  0.0835, -0.0361,  ..., -0.3688, -0.1076,  0.1674],\n",
            "         [-0.1638, -0.0032, -0.0184,  ..., -0.2671, -0.0349,  0.1150],\n",
            "         [-0.2148,  0.0256, -0.0084,  ..., -0.3557, -0.1940,  0.1494],\n",
            "         ...,\n",
            "         [-0.1583, -0.1155, -0.0508,  ..., -0.3311, -0.0430,  0.1531],\n",
            "         [-0.2012, -0.1283,  0.0229,  ..., -0.3077, -0.0372,  0.0868],\n",
            "         [-0.1430, -0.1541, -0.0413,  ..., -0.3310, -0.0943,  0.0912]],\n",
            "\n",
            "        [[-0.0045, -0.1069,  0.0222,  ...,  0.1152, -0.1037,  0.1929],\n",
            "         [-0.0871,  0.0307, -0.0510,  ...,  0.0813, -0.1563,  0.1665],\n",
            "         [-0.0306,  0.0541, -0.0451,  ...,  0.0170, -0.1591,  0.1777],\n",
            "         ...,\n",
            "         [-0.0077,  0.0106, -0.1146,  ...,  0.0133, -0.1399,  0.1668],\n",
            "         [-0.0353, -0.0470, -0.1340,  ...,  0.0859, -0.0266,  0.1576],\n",
            "         [ 0.0167, -0.0306, -0.1005,  ...,  0.0686, -0.1148,  0.1574]]],\n",
            "       grad_fn=<BmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, dim_input, dim_q, dim_v):\n",
        "        '''\n",
        "        dim_input: the dimension of each sample\n",
        "        dim_q: dimension of Q matrix, should be equal to dim_k\n",
        "        dim_v: dimension of V matrix, also the  dimension of the attention output\n",
        "        '''\n",
        "        super(SelfAttention, self).__init__()\n",
        "\n",
        "        self.dim_input = dim_input\n",
        "        self.dim_q = dim_q\n",
        "        self.dim_k = dim_q\n",
        "        self.dim_v = dim_v\n",
        "\n",
        "        # Define the linear projection\n",
        "        self.linear_q = nn.Linear(self.dim_input, self.dim_q, bias=False)\n",
        "        self.linear_k = nn.Linear(self.dim_input, self.dim_k, bias=False)\n",
        "        self.linear_v = nn.Linear(self.dim_input, self.dim_v, bias=False)\n",
        "        self._norm_fact = 1 / math.sqrt(self.dim_k)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, n, dim_q = x.shape\n",
        "\n",
        "        q = self.linear_q(x) # (batchsize, seq_len, dim_q)\n",
        "        k = self.linear_k(x) # (batchsize, seq_len, dim_k)\n",
        "        v = self.linear_v(x) # (batchsize, seq_len, dim_v)\n",
        "        print(f'x.shape:{x.shape} \\n Q.shape:{q.shape} \\n K.shape:{k.shape} \\n V.shape:{v.shape}')\n",
        "\n",
        "        dist = torch.bmm(q, k.transpose(1,2)) * self._norm_fact\n",
        "        dist = torch.softmax(dist, dim=-1)\n",
        "        print('attention matrix: ', dist.shape)\n",
        "\n",
        "        outputs = torch.bmm(dist, v)\n",
        "        print('attention outputs: ', outputs.shape)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "batch_size = 32 # number of samples in a batch\n",
        "dim_input = 128 # dimension of each item in the sample sequence\n",
        "seq_len = 20 # sequence length for each sample\n",
        "x = torch.randn(batch_size, seq_len, dim_input)\n",
        "self_attention = SelfAttention(dim_input, dim_q = 64, dim_v = 32)\n",
        "\n",
        "attention = self_attention(x)\n",
        "\n",
        "print(attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZaAFL8MS2Ng"
      },
      "source": [
        "# Transformers\n",
        "In this section, we implement a 6-layer Vision Transformer (ViT) and trained it on the MNIST dataset.\n",
        "We consider the classification tasks.\n",
        "First, we load the MNIST dataset as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "rZ-eIaeZjWjL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torchvision import datasets, utils\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "def get_mnist_loader(batch_size=100, shuffle=True):\n",
        "    \"\"\"\n",
        "\n",
        "    :return: train_loader, test_loader\n",
        "    \"\"\"\n",
        "    train_dataset = MNIST(root='./data',\n",
        "                          train=True,\n",
        "                          transform=torchvision.transforms.ToTensor(),\n",
        "                          download=True)\n",
        "    test_dataset = MNIST(root='./data',\n",
        "                         train=False,\n",
        "                         transform=torchvision.transforms.ToTensor(),\n",
        "                         download=True)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=shuffle)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                              batch_size=batch_size,\n",
        "                                              shuffle=False)\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-C06IoPIjePg"
      },
      "outputs": [],
      "source": [
        "# This package is needed to build the transformer\n",
        "# !pip install einops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx9eZrMpmA2z"
      },
      "source": [
        "## Build ViT from scratch\n",
        "Recall that each Transformer block include 2 modules: the self-attention module, the feedforward module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Vr6d7IWfjpxY"
      },
      "outputs": [],
      "source": [
        "from einops import rearrange\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads=8):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.scale = dim ** -0.5\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
        "        self.to_out = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "        b, n, _, h = *x.shape, self.heads\n",
        "        qkv = self.to_qkv(x)\n",
        "        q, k, v = rearrange(qkv, 'b n (qkv h d) -> qkv b h n d', qkv=3, h=h)\n",
        "\n",
        "        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = F.pad(mask.flatten(1), (1, 0), value = True)\n",
        "            assert mask.shape[-1] == dots.shape[-1], 'mask has incorrect dimensions'\n",
        "            mask = mask[:, None, :] * mask[:, :, None]\n",
        "            dots.masked_fill_(~mask, float('-inf'))\n",
        "            del mask\n",
        "\n",
        "        attn = dots.softmax(dim=-1)\n",
        "\n",
        "        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out =  self.to_out(out)\n",
        "        return out\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                Residual(PreNorm(dim, Attention(dim, heads = heads))),\n",
        "                Residual(PreNorm(dim, FeedForward(dim, mlp_dim)))\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x, mask=mask)\n",
        "            x = ff(x)\n",
        "        return x\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels=3):\n",
        "        super().__init__()\n",
        "        assert image_size % patch_size == 0, 'image dimensions must be divisible by the patch size'\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        patch_dim = channels * patch_size ** 2\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.transformer = Transformer(dim, depth, heads, mlp_dim)\n",
        "\n",
        "        self.to_cls_token = nn.Identity()\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_dim),\n",
        "            nn.GELU(), # Gaussian Error Linear Units is another type of activation function\n",
        "            nn.Linear(mlp_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img, mask=None):\n",
        "        p = self.patch_size\n",
        "\n",
        "        x = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
        "        x = self.patch_to_embedding(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(img.shape[0], -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.pos_embedding\n",
        "        x = self.transformer(x, mask)\n",
        "\n",
        "        x = self.to_cls_token(x[:, 0])\n",
        "        return self.mlp_head(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTawNC64mhBO"
      },
      "source": [
        "## Training and test function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "rKJ4tjCjjycH"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, optimizer, data_loader, loss_history):\n",
        "    total_samples = len(data_loader.dataset)\n",
        "    model.train()\n",
        "\n",
        "    for i, (data, target) in enumerate(data_loader):\n",
        "        # data = data.cuda()\n",
        "        # target = target.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        output = F.log_softmax(model(data), dim=1)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print('[' +  '{:5}'.format(i * len(data)) + '/' + '{:5}'.format(total_samples) +\n",
        "                  ' (' + '{:3.0f}'.format(100 * i / len(data_loader)) + '%)]  Loss: ' +\n",
        "                  '{:6.4f}'.format(loss.item()))\n",
        "        loss_history.append(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "vph2CrNxj6ZZ"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data_loader, loss_history):\n",
        "    model.eval()\n",
        "\n",
        "    total_samples = len(data_loader.dataset)\n",
        "    correct_samples = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    # We do not need to remember the gradients when testing\n",
        "    # This will help reduce memory\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            # data = data.cuda()\n",
        "            # target = target.cuda()\n",
        "            output = F.log_softmax(model(data), dim=1)\n",
        "            loss = F.nll_loss(output, target, reduction='sum')\n",
        "            _, pred = torch.max(output, dim=1)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            correct_samples += pred.eq(target).sum()\n",
        "            loss_history.append(loss.item())\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    # loss_history.append(avg_loss)\n",
        "    print('\\nAverage test loss: ' + '{:.4f}'.format(avg_loss) +\n",
        "          '  Accuracy:' + '{:5}'.format(correct_samples) + '/' +\n",
        "          '{:5}'.format(total_samples) + ' (' +\n",
        "          '{:4.2f}'.format(100.0 * correct_samples / total_samples) + '%)\\n')\n",
        "    return correct_samples / total_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRYys50km0-E"
      },
      "source": [
        "## Let's start training!\n",
        "Here, you can change the ViT structure by changing the hyper-parametrs inside ``ViT`` function.\n",
        "The default settings are with 6 layers, 8 heads for the multi-head attention mechanism and embedding dimension of 64.\n",
        "You can also increase the number of epochs to obtain better results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "rVLJLLDuj7yQ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# You can change the architecture here\n",
        "model = ViT(image_size=28, patch_size=7, num_classes=10, channels=1,\n",
        "            dim=64, depth=6, heads=8, mlp_dim=128)\n",
        "# model = model.cuda()\n",
        "# We also print the network architecture\n",
        "model\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "train_loss_history, test_loss_history = [], []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Vlt3tk-MkDB9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.3848\n",
            "[12800/60000 ( 21%)]  Loss: 0.9746\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[37], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, N_EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch:\u001b[39m\u001b[38;5;124m'\u001b[39m, epoch,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLR:\u001b[39m\u001b[38;5;124m'\u001b[39m, scheduler\u001b[38;5;241m.\u001b[39mget_last_lr())\n\u001b[1;32m---> 11\u001b[0m     \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss_history\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     evaluate(model, test_loader, test_loss_history)\n\u001b[0;32m     13\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
            "Cell \u001b[1;32mIn[26], line 9\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, optimizer, data_loader, loss_history)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# data = data.cuda()\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# target = target.cuda()\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 9\u001b[0m     output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     10\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(output, target)\n\u001b[0;32m     11\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
            "File \u001b[1;32mc:\\Git\\kul-anndl-ss24\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Git\\kul-anndl-ss24\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[25], line 111\u001b[0m, in \u001b[0;36mViT.forward\u001b[1;34m(self, img, mask)\u001b[0m\n\u001b[0;32m    109\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((cls_tokens, x), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    110\u001b[0m x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding\n\u001b[1;32m--> 111\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_cls_token(x[:, \u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_head(x)\n",
            "File \u001b[1;32mc:\\Git\\kul-anndl-ss24\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Git\\kul-anndl-ss24\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[25], line 76\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attn, ff \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 76\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m         x \u001b[38;5;241m=\u001b[39m ff(x)\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[1;32mc:\\Git\\kul-anndl-ss24\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Git\\kul-anndl-ss24\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[25], line 11\u001b[0m, in \u001b[0;36mResidual.forward\u001b[1;34m(self, x, **kwargs)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m x\n",
            "File \u001b[1;32mc:\\Git\\kul-anndl-ss24\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Git\\kul-anndl-ss24\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[25], line 20\u001b[0m, in \u001b[0;36mPreNorm.forward\u001b[1;34m(self, x, **kwargs)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Git\\kul-anndl-ss24\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Git\\kul-anndl-ss24\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[25], line 45\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     44\u001b[0m     b, n, _, h \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m*\u001b[39mx\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads\n\u001b[1;32m---> 45\u001b[0m     qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_qkv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m rearrange(qkv, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb n (qkv h d) -> qkv b h n d\u001b[39m\u001b[38;5;124m'\u001b[39m, qkv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, h\u001b[38;5;241m=\u001b[39mh)\n\u001b[0;32m     48\u001b[0m     dots \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbhid,bhjd->bhij\u001b[39m\u001b[38;5;124m'\u001b[39m, q, k) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n",
            "File \u001b[1;32mc:\\Git\\kul-anndl-ss24\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Git\\kul-anndl-ss24\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Git\\kul-anndl-ss24\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 20\n",
        "\n",
        "train_loader, test_loader = get_mnist_loader(batch_size=128, shuffle=True)\n",
        "\n",
        "# Gradually reduce the learning rate while training\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    print('Epoch:', epoch,'LR:', scheduler.get_last_lr())\n",
        "    train_epoch(model, optimizer, train_loader, train_loss_history)\n",
        "    evaluate(model, test_loader, test_loss_history)\n",
        "    scheduler.step()\n",
        "\n",
        "print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "XicoRf8_nUTK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------\n",
            "Running experiment with dim=256, depth=8, heads=8, mlp_dim=128\n",
            "Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.3744\n",
            "[12800/60000 ( 21%)]  Loss: 0.3310\n",
            "[25600/60000 ( 43%)]  Loss: 0.3602\n",
            "[38400/60000 ( 64%)]  Loss: 0.1913\n",
            "[51200/60000 ( 85%)]  Loss: 0.2411\n",
            "\n",
            "Average test loss: 0.1320  Accuracy: 9577/10000 (95.77%)\n",
            "\n",
            "Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.0653\n",
            "[12800/60000 ( 21%)]  Loss: 0.1206\n",
            "[25600/60000 ( 43%)]  Loss: 0.0697\n",
            "[38400/60000 ( 64%)]  Loss: 0.1285\n",
            "[51200/60000 ( 85%)]  Loss: 0.1127\n",
            "\n",
            "Average test loss: 0.1178  Accuracy: 9622/10000 (96.22%)\n",
            "\n",
            "Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.1398\n",
            "[12800/60000 ( 21%)]  Loss: 0.0879\n",
            "[25600/60000 ( 43%)]  Loss: 0.0674\n",
            "[38400/60000 ( 64%)]  Loss: 0.1428\n",
            "[51200/60000 ( 85%)]  Loss: 0.0757\n",
            "\n",
            "Average test loss: 0.0890  Accuracy: 9716/10000 (97.16%)\n",
            "\n",
            "Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.0569\n",
            "[12800/60000 ( 21%)]  Loss: 0.0429\n",
            "[25600/60000 ( 43%)]  Loss: 0.1217\n",
            "[38400/60000 ( 64%)]  Loss: 0.0288\n",
            "[51200/60000 ( 85%)]  Loss: 0.0167\n",
            "\n",
            "Average test loss: 0.1000  Accuracy: 9715/10000 (97.15%)\n",
            "\n",
            "Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.0656\n",
            "[12800/60000 ( 21%)]  Loss: 0.0943\n",
            "[25600/60000 ( 43%)]  Loss: 0.0473\n",
            "[38400/60000 ( 64%)]  Loss: 0.1002\n",
            "[51200/60000 ( 85%)]  Loss: 0.1139\n",
            "\n",
            "Average test loss: 0.0644  Accuracy: 9812/10000 (98.12%)\n",
            "\n",
            "Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.0415\n",
            "[12800/60000 ( 21%)]  Loss: 0.0542\n",
            "[25600/60000 ( 43%)]  Loss: 0.1036\n",
            "[38400/60000 ( 64%)]  Loss: 0.1514\n",
            "[51200/60000 ( 85%)]  Loss: 0.1287\n",
            "\n",
            "Average test loss: 0.0736  Accuracy: 9759/10000 (97.59%)\n",
            "\n",
            "Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.1109\n",
            "[12800/60000 ( 21%)]  Loss: 0.0528\n",
            "[25600/60000 ( 43%)]  Loss: 0.0150\n",
            "[38400/60000 ( 64%)]  Loss: 0.0692\n",
            "[51200/60000 ( 85%)]  Loss: 0.0356\n",
            "\n",
            "Average test loss: 0.0749  Accuracy: 9787/10000 (97.87%)\n",
            "\n",
            "Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.0507\n",
            "[12800/60000 ( 21%)]  Loss: 0.0593\n",
            "[25600/60000 ( 43%)]  Loss: 0.0293\n",
            "[38400/60000 ( 64%)]  Loss: 0.0973\n",
            "[51200/60000 ( 85%)]  Loss: 0.0358\n",
            "\n",
            "Average test loss: 0.0766  Accuracy: 9776/10000 (97.76%)\n",
            "\n",
            "Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0814\n",
            "[12800/60000 ( 21%)]  Loss: 0.0114\n",
            "[25600/60000 ( 43%)]  Loss: 0.0480\n",
            "[38400/60000 ( 64%)]  Loss: 0.1190\n",
            "[51200/60000 ( 85%)]  Loss: 0.0913\n",
            "\n",
            "Average test loss: 0.0616  Accuracy: 9810/10000 (98.10%)\n",
            "\n",
            "Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0111\n",
            "[12800/60000 ( 21%)]  Loss: 0.0393\n",
            "[25600/60000 ( 43%)]  Loss: 0.0611\n",
            "[38400/60000 ( 64%)]  Loss: 0.0069\n",
            "[51200/60000 ( 85%)]  Loss: 0.0445\n",
            "\n",
            "Average test loss: 0.0765  Accuracy: 9764/10000 (97.64%)\n",
            "\n",
            "Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.0435\n",
            "[12800/60000 ( 21%)]  Loss: 0.0555\n",
            "[25600/60000 ( 43%)]  Loss: 0.1251\n",
            "[38400/60000 ( 64%)]  Loss: 0.0072\n",
            "[51200/60000 ( 85%)]  Loss: 0.0104\n",
            "\n",
            "Average test loss: 0.0565  Accuracy: 9842/10000 (98.42%)\n",
            "\n",
            "Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0247\n",
            "[12800/60000 ( 21%)]  Loss: 0.0111\n",
            "[25600/60000 ( 43%)]  Loss: 0.0070\n",
            "[38400/60000 ( 64%)]  Loss: 0.0199\n",
            "[51200/60000 ( 85%)]  Loss: 0.0042\n",
            "\n",
            "Average test loss: 0.0763  Accuracy: 9801/10000 (98.01%)\n",
            "\n",
            "Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0391\n",
            "[12800/60000 ( 21%)]  Loss: 0.0558\n",
            "[25600/60000 ( 43%)]  Loss: 0.0375\n",
            "[38400/60000 ( 64%)]  Loss: 0.0290\n",
            "[51200/60000 ( 85%)]  Loss: 0.0313\n",
            "\n",
            "Average test loss: 0.0924  Accuracy: 9765/10000 (97.65%)\n",
            "\n",
            "Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0509\n",
            "[12800/60000 ( 21%)]  Loss: 0.0061\n",
            "[25600/60000 ( 43%)]  Loss: 0.0210\n",
            "[38400/60000 ( 64%)]  Loss: 0.0339\n",
            "[51200/60000 ( 85%)]  Loss: 0.0731\n",
            "\n",
            "Average test loss: 0.0791  Accuracy: 9813/10000 (98.13%)\n",
            "\n",
            "Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0468\n",
            "[12800/60000 ( 21%)]  Loss: 0.0298\n",
            "[25600/60000 ( 43%)]  Loss: 0.0010\n",
            "[38400/60000 ( 64%)]  Loss: 0.0067\n",
            "[51200/60000 ( 85%)]  Loss: 0.0035\n",
            "\n",
            "Average test loss: 0.0674  Accuracy: 9838/10000 (98.38%)\n",
            "\n",
            "Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0090\n",
            "[12800/60000 ( 21%)]  Loss: 0.0036\n",
            "[25600/60000 ( 43%)]  Loss: 0.0242\n",
            "[38400/60000 ( 64%)]  Loss: 0.0058\n",
            "[51200/60000 ( 85%)]  Loss: 0.0063\n",
            "\n",
            "Average test loss: 0.0550  Accuracy: 9858/10000 (98.58%)\n",
            "\n",
            "Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0070\n",
            "[12800/60000 ( 21%)]  Loss: 0.0020\n",
            "[25600/60000 ( 43%)]  Loss: 0.0014\n",
            "[38400/60000 ( 64%)]  Loss: 0.0162\n",
            "[51200/60000 ( 85%)]  Loss: 0.0262\n",
            "\n",
            "Average test loss: 0.0670  Accuracy: 9837/10000 (98.37%)\n",
            "\n",
            "Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0165\n",
            "[12800/60000 ( 21%)]  Loss: 0.0460\n",
            "[25600/60000 ( 43%)]  Loss: 0.0079\n",
            "[38400/60000 ( 64%)]  Loss: 0.0089\n",
            "[51200/60000 ( 85%)]  Loss: 0.0074\n",
            "\n",
            "Average test loss: 0.0643  Accuracy: 9839/10000 (98.39%)\n",
            "\n",
            "Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0100\n",
            "[12800/60000 ( 21%)]  Loss: 0.0022\n",
            "[25600/60000 ( 43%)]  Loss: 0.0065\n",
            "[38400/60000 ( 64%)]  Loss: 0.0007\n",
            "[51200/60000 ( 85%)]  Loss: 0.0373\n",
            "\n",
            "Average test loss: 0.0677  Accuracy: 9866/10000 (98.66%)\n",
            "\n",
            "Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0004\n",
            "[12800/60000 ( 21%)]  Loss: 0.0011\n",
            "[25600/60000 ( 43%)]  Loss: 0.0064\n",
            "[38400/60000 ( 64%)]  Loss: 0.0517\n",
            "[51200/60000 ( 85%)]  Loss: 0.0117\n",
            "\n",
            "Average test loss: 0.0787  Accuracy: 9827/10000 (98.27%)\n",
            "\n",
            "Execution time: 1067.33 seconds\n",
            "-----------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------\n",
            "Running experiment with dim=128, depth=10, heads=8, mlp_dim=64\n",
            "Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.3957\n",
            "[12800/60000 ( 21%)]  Loss: 0.6035\n",
            "[25600/60000 ( 43%)]  Loss: 0.3588\n",
            "[38400/60000 ( 64%)]  Loss: 0.2632\n",
            "[51200/60000 ( 85%)]  Loss: 0.1854\n",
            "\n",
            "Average test loss: 0.1665  Accuracy: 9490/10000 (94.90%)\n",
            "\n",
            "Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.3280\n",
            "[12800/60000 ( 21%)]  Loss: 0.0949\n",
            "[25600/60000 ( 43%)]  Loss: 0.1033\n",
            "[38400/60000 ( 64%)]  Loss: 0.1764\n",
            "[51200/60000 ( 85%)]  Loss: 0.1137\n",
            "\n",
            "Average test loss: 0.1156  Accuracy: 9624/10000 (96.24%)\n",
            "\n",
            "Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.1041\n",
            "[12800/60000 ( 21%)]  Loss: 0.0990\n",
            "[25600/60000 ( 43%)]  Loss: 0.0637\n",
            "[38400/60000 ( 64%)]  Loss: 0.0864\n",
            "[51200/60000 ( 85%)]  Loss: 0.0862\n",
            "\n",
            "Average test loss: 0.1071  Accuracy: 9695/10000 (96.95%)\n",
            "\n",
            "Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.0254\n",
            "[12800/60000 ( 21%)]  Loss: 0.0156\n",
            "[25600/60000 ( 43%)]  Loss: 0.0192\n",
            "[38400/60000 ( 64%)]  Loss: 0.0457\n",
            "[51200/60000 ( 85%)]  Loss: 0.0982\n",
            "\n",
            "Average test loss: 0.0799  Accuracy: 9763/10000 (97.63%)\n",
            "\n",
            "Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.0436\n",
            "[12800/60000 ( 21%)]  Loss: 0.0182\n",
            "[25600/60000 ( 43%)]  Loss: 0.1213\n",
            "[38400/60000 ( 64%)]  Loss: 0.0389\n",
            "[51200/60000 ( 85%)]  Loss: 0.0229\n",
            "\n",
            "Average test loss: 0.0750  Accuracy: 9768/10000 (97.68%)\n",
            "\n",
            "Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.0842\n",
            "[12800/60000 ( 21%)]  Loss: 0.0089\n",
            "[25600/60000 ( 43%)]  Loss: 0.0450\n",
            "[38400/60000 ( 64%)]  Loss: 0.0592\n",
            "[51200/60000 ( 85%)]  Loss: 0.0679\n",
            "\n",
            "Average test loss: 0.0771  Accuracy: 9777/10000 (97.77%)\n",
            "\n",
            "Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.0558\n",
            "[12800/60000 ( 21%)]  Loss: 0.0411\n",
            "[25600/60000 ( 43%)]  Loss: 0.0553\n",
            "[38400/60000 ( 64%)]  Loss: 0.0210\n",
            "[51200/60000 ( 85%)]  Loss: 0.0262\n",
            "\n",
            "Average test loss: 0.0657  Accuracy: 9822/10000 (98.22%)\n",
            "\n",
            "Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.0092\n",
            "[12800/60000 ( 21%)]  Loss: 0.0188\n",
            "[25600/60000 ( 43%)]  Loss: 0.0292\n",
            "[38400/60000 ( 64%)]  Loss: 0.0240\n",
            "[51200/60000 ( 85%)]  Loss: 0.0120\n",
            "\n",
            "Average test loss: 0.0575  Accuracy: 9835/10000 (98.35%)\n",
            "\n",
            "Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0024\n",
            "[12800/60000 ( 21%)]  Loss: 0.0122\n",
            "[25600/60000 ( 43%)]  Loss: 0.0290\n",
            "[38400/60000 ( 64%)]  Loss: 0.0047\n",
            "[51200/60000 ( 85%)]  Loss: 0.0652\n",
            "\n",
            "Average test loss: 0.0704  Accuracy: 9798/10000 (97.98%)\n",
            "\n",
            "Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0154\n",
            "[12800/60000 ( 21%)]  Loss: 0.0099\n",
            "[25600/60000 ( 43%)]  Loss: 0.0050\n",
            "[38400/60000 ( 64%)]  Loss: 0.0060\n",
            "[51200/60000 ( 85%)]  Loss: 0.0633\n",
            "\n",
            "Average test loss: 0.0687  Accuracy: 9824/10000 (98.24%)\n",
            "\n",
            "Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.0202\n",
            "[12800/60000 ( 21%)]  Loss: 0.0031\n",
            "[25600/60000 ( 43%)]  Loss: 0.0273\n",
            "[38400/60000 ( 64%)]  Loss: 0.0116\n",
            "[51200/60000 ( 85%)]  Loss: 0.0483\n",
            "\n",
            "Average test loss: 0.0812  Accuracy: 9799/10000 (97.99%)\n",
            "\n",
            "Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0056\n",
            "[12800/60000 ( 21%)]  Loss: 0.0006\n",
            "[25600/60000 ( 43%)]  Loss: 0.0051\n",
            "[38400/60000 ( 64%)]  Loss: 0.0782\n",
            "[51200/60000 ( 85%)]  Loss: 0.0046\n",
            "\n",
            "Average test loss: 0.0706  Accuracy: 9829/10000 (98.29%)\n",
            "\n",
            "Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0075\n",
            "[12800/60000 ( 21%)]  Loss: 0.0077\n",
            "[25600/60000 ( 43%)]  Loss: 0.0236\n",
            "[38400/60000 ( 64%)]  Loss: 0.0117\n",
            "[51200/60000 ( 85%)]  Loss: 0.0005\n",
            "\n",
            "Average test loss: 0.0794  Accuracy: 9825/10000 (98.25%)\n",
            "\n",
            "Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0094\n",
            "[12800/60000 ( 21%)]  Loss: 0.0085\n",
            "[25600/60000 ( 43%)]  Loss: 0.0116\n",
            "[38400/60000 ( 64%)]  Loss: 0.0059\n",
            "[51200/60000 ( 85%)]  Loss: 0.0204\n",
            "\n",
            "Average test loss: 0.0637  Accuracy: 9858/10000 (98.58%)\n",
            "\n",
            "Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0058\n",
            "[12800/60000 ( 21%)]  Loss: 0.0128\n",
            "[25600/60000 ( 43%)]  Loss: 0.0018\n",
            "[38400/60000 ( 64%)]  Loss: 0.0006\n",
            "[51200/60000 ( 85%)]  Loss: 0.0022\n",
            "\n",
            "Average test loss: 0.0637  Accuracy: 9850/10000 (98.50%)\n",
            "\n",
            "Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0008\n",
            "[12800/60000 ( 21%)]  Loss: 0.0149\n",
            "[25600/60000 ( 43%)]  Loss: 0.0003\n",
            "[38400/60000 ( 64%)]  Loss: 0.0637\n",
            "[51200/60000 ( 85%)]  Loss: 0.1021\n",
            "\n",
            "Average test loss: 0.0705  Accuracy: 9829/10000 (98.29%)\n",
            "\n",
            "Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0051\n",
            "[12800/60000 ( 21%)]  Loss: 0.0015\n",
            "[25600/60000 ( 43%)]  Loss: 0.0001\n",
            "[38400/60000 ( 64%)]  Loss: 0.0006\n",
            "[51200/60000 ( 85%)]  Loss: 0.0228\n",
            "\n",
            "Average test loss: 0.0805  Accuracy: 9823/10000 (98.23%)\n",
            "\n",
            "Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0024\n",
            "[12800/60000 ( 21%)]  Loss: 0.0025\n",
            "[25600/60000 ( 43%)]  Loss: 0.0504\n",
            "[38400/60000 ( 64%)]  Loss: 0.0005\n",
            "[51200/60000 ( 85%)]  Loss: 0.0028\n",
            "\n",
            "Average test loss: 0.0775  Accuracy: 9840/10000 (98.40%)\n",
            "\n",
            "Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0010\n",
            "[12800/60000 ( 21%)]  Loss: 0.0008\n",
            "[25600/60000 ( 43%)]  Loss: 0.0009\n",
            "[38400/60000 ( 64%)]  Loss: 0.0023\n",
            "[51200/60000 ( 85%)]  Loss: 0.0174\n",
            "\n",
            "Average test loss: 0.0871  Accuracy: 9827/10000 (98.27%)\n",
            "\n",
            "Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0032\n",
            "[12800/60000 ( 21%)]  Loss: 0.0012\n",
            "[25600/60000 ( 43%)]  Loss: 0.0074\n",
            "[38400/60000 ( 64%)]  Loss: 0.0041\n",
            "[51200/60000 ( 85%)]  Loss: 0.0048\n",
            "\n",
            "Average test loss: 0.0781  Accuracy: 9838/10000 (98.38%)\n",
            "\n",
            "Execution time: 710.57 seconds\n",
            "-----------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------\n",
            "Running experiment with dim=32, depth=6, heads=8, mlp_dim=512\n",
            "Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.3838\n",
            "[12800/60000 ( 21%)]  Loss: 1.3773\n",
            "[25600/60000 ( 43%)]  Loss: 0.9920\n",
            "[38400/60000 ( 64%)]  Loss: 0.7411\n",
            "[51200/60000 ( 85%)]  Loss: 0.3624\n",
            "\n",
            "Average test loss: 0.5010  Accuracy: 8319/10000 (83.19%)\n",
            "\n",
            "Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.4544\n",
            "[12800/60000 ( 21%)]  Loss: 0.3027\n",
            "[25600/60000 ( 43%)]  Loss: 0.3011\n",
            "[38400/60000 ( 64%)]  Loss: 0.3217\n",
            "[51200/60000 ( 85%)]  Loss: 0.0967\n",
            "\n",
            "Average test loss: 0.1971  Accuracy: 9387/10000 (93.87%)\n",
            "\n",
            "Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.2417\n",
            "[12800/60000 ( 21%)]  Loss: 0.2029\n",
            "[25600/60000 ( 43%)]  Loss: 0.1114\n",
            "[38400/60000 ( 64%)]  Loss: 0.1259\n",
            "[51200/60000 ( 85%)]  Loss: 0.1070\n",
            "\n",
            "Average test loss: 0.1283  Accuracy: 9576/10000 (95.76%)\n",
            "\n",
            "Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.1346\n",
            "[12800/60000 ( 21%)]  Loss: 0.1474\n",
            "[25600/60000 ( 43%)]  Loss: 0.0960\n",
            "[38400/60000 ( 64%)]  Loss: 0.0800\n",
            "[51200/60000 ( 85%)]  Loss: 0.2336\n",
            "\n",
            "Average test loss: 0.1299  Accuracy: 9589/10000 (95.89%)\n",
            "\n",
            "Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.0886\n",
            "[12800/60000 ( 21%)]  Loss: 0.1197\n",
            "[25600/60000 ( 43%)]  Loss: 0.1646\n",
            "[38400/60000 ( 64%)]  Loss: 0.2052\n",
            "[51200/60000 ( 85%)]  Loss: 0.0500\n",
            "\n",
            "Average test loss: 0.1246  Accuracy: 9620/10000 (96.20%)\n",
            "\n",
            "Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.1591\n",
            "[12800/60000 ( 21%)]  Loss: 0.1013\n",
            "[25600/60000 ( 43%)]  Loss: 0.1380\n",
            "[38400/60000 ( 64%)]  Loss: 0.0431\n",
            "[51200/60000 ( 85%)]  Loss: 0.1081\n",
            "\n",
            "Average test loss: 0.0963  Accuracy: 9690/10000 (96.90%)\n",
            "\n",
            "Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.0742\n",
            "[12800/60000 ( 21%)]  Loss: 0.0216\n",
            "[25600/60000 ( 43%)]  Loss: 0.0512\n",
            "[38400/60000 ( 64%)]  Loss: 0.1172\n",
            "[51200/60000 ( 85%)]  Loss: 0.0979\n",
            "\n",
            "Average test loss: 0.0930  Accuracy: 9702/10000 (97.02%)\n",
            "\n",
            "Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.0657\n",
            "[12800/60000 ( 21%)]  Loss: 0.0312\n",
            "[25600/60000 ( 43%)]  Loss: 0.0858\n",
            "[38400/60000 ( 64%)]  Loss: 0.0563\n",
            "[51200/60000 ( 85%)]  Loss: 0.0456\n",
            "\n",
            "Average test loss: 0.0844  Accuracy: 9743/10000 (97.43%)\n",
            "\n",
            "Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0668\n",
            "[12800/60000 ( 21%)]  Loss: 0.0246\n",
            "[25600/60000 ( 43%)]  Loss: 0.0429\n",
            "[38400/60000 ( 64%)]  Loss: 0.0494\n",
            "[51200/60000 ( 85%)]  Loss: 0.1256\n",
            "\n",
            "Average test loss: 0.0971  Accuracy: 9725/10000 (97.25%)\n",
            "\n",
            "Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0601\n",
            "[12800/60000 ( 21%)]  Loss: 0.0455\n",
            "[25600/60000 ( 43%)]  Loss: 0.0589\n",
            "[38400/60000 ( 64%)]  Loss: 0.0615\n",
            "[51200/60000 ( 85%)]  Loss: 0.0456\n",
            "\n",
            "Average test loss: 0.0765  Accuracy: 9767/10000 (97.67%)\n",
            "\n",
            "Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.0804\n",
            "[12800/60000 ( 21%)]  Loss: 0.0558\n",
            "[25600/60000 ( 43%)]  Loss: 0.0222\n",
            "[38400/60000 ( 64%)]  Loss: 0.0614\n",
            "[51200/60000 ( 85%)]  Loss: 0.0288\n",
            "\n",
            "Average test loss: 0.0822  Accuracy: 9752/10000 (97.52%)\n",
            "\n",
            "Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0326\n",
            "[12800/60000 ( 21%)]  Loss: 0.0211\n",
            "[25600/60000 ( 43%)]  Loss: 0.0197\n",
            "[38400/60000 ( 64%)]  Loss: 0.0065\n",
            "[51200/60000 ( 85%)]  Loss: 0.0517\n",
            "\n",
            "Average test loss: 0.0751  Accuracy: 9774/10000 (97.74%)\n",
            "\n",
            "Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0443\n",
            "[12800/60000 ( 21%)]  Loss: 0.0906\n",
            "[25600/60000 ( 43%)]  Loss: 0.0495\n",
            "[38400/60000 ( 64%)]  Loss: 0.0142\n",
            "[51200/60000 ( 85%)]  Loss: 0.0178\n",
            "\n",
            "Average test loss: 0.0823  Accuracy: 9757/10000 (97.57%)\n",
            "\n",
            "Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0433\n",
            "[12800/60000 ( 21%)]  Loss: 0.0259\n",
            "[25600/60000 ( 43%)]  Loss: 0.0572\n",
            "[38400/60000 ( 64%)]  Loss: 0.0846\n",
            "[51200/60000 ( 85%)]  Loss: 0.0139\n",
            "\n",
            "Average test loss: 0.0709  Accuracy: 9802/10000 (98.02%)\n",
            "\n",
            "Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0217\n",
            "[12800/60000 ( 21%)]  Loss: 0.0489\n",
            "[25600/60000 ( 43%)]  Loss: 0.0507\n",
            "[38400/60000 ( 64%)]  Loss: 0.0334\n",
            "[51200/60000 ( 85%)]  Loss: 0.0066\n",
            "\n",
            "Average test loss: 0.0722  Accuracy: 9785/10000 (97.85%)\n",
            "\n",
            "Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0143\n",
            "[12800/60000 ( 21%)]  Loss: 0.0177\n",
            "[25600/60000 ( 43%)]  Loss: 0.0109\n",
            "[38400/60000 ( 64%)]  Loss: 0.0097\n",
            "[51200/60000 ( 85%)]  Loss: 0.0588\n",
            "\n",
            "Average test loss: 0.0658  Accuracy: 9812/10000 (98.12%)\n",
            "\n",
            "Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0227\n",
            "[12800/60000 ( 21%)]  Loss: 0.0450\n",
            "[25600/60000 ( 43%)]  Loss: 0.0281\n",
            "[38400/60000 ( 64%)]  Loss: 0.0671\n",
            "[51200/60000 ( 85%)]  Loss: 0.0076\n",
            "\n",
            "Average test loss: 0.0743  Accuracy: 9810/10000 (98.10%)\n",
            "\n",
            "Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0327\n",
            "[12800/60000 ( 21%)]  Loss: 0.0048\n",
            "[25600/60000 ( 43%)]  Loss: 0.0067\n",
            "[38400/60000 ( 64%)]  Loss: 0.0072\n",
            "[51200/60000 ( 85%)]  Loss: 0.0055\n",
            "\n",
            "Average test loss: 0.0774  Accuracy: 9784/10000 (97.84%)\n",
            "\n",
            "Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0341\n",
            "[12800/60000 ( 21%)]  Loss: 0.0122\n",
            "[25600/60000 ( 43%)]  Loss: 0.0029\n",
            "[38400/60000 ( 64%)]  Loss: 0.0103\n",
            "[51200/60000 ( 85%)]  Loss: 0.0058\n",
            "\n",
            "Average test loss: 0.0814  Accuracy: 9797/10000 (97.97%)\n",
            "\n",
            "Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0064\n",
            "[12800/60000 ( 21%)]  Loss: 0.0080\n",
            "[25600/60000 ( 43%)]  Loss: 0.0049\n",
            "[38400/60000 ( 64%)]  Loss: 0.0015\n",
            "[51200/60000 ( 85%)]  Loss: 0.0131\n",
            "\n",
            "Average test loss: 0.0867  Accuracy: 9778/10000 (97.78%)\n",
            "\n",
            "Execution time: 418.56 seconds\n",
            "-----------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------\n",
            "Running experiment with dim=256, depth=4, heads=8, mlp_dim=512\n",
            "Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.3535\n",
            "[12800/60000 ( 21%)]  Loss: 0.2610\n",
            "[25600/60000 ( 43%)]  Loss: 0.2591\n",
            "[38400/60000 ( 64%)]  Loss: 0.0693\n",
            "[51200/60000 ( 85%)]  Loss: 0.1774\n",
            "\n",
            "Average test loss: 0.1679  Accuracy: 9463/10000 (94.63%)\n",
            "\n",
            "Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.2754\n",
            "[12800/60000 ( 21%)]  Loss: 0.1166\n",
            "[25600/60000 ( 43%)]  Loss: 0.0693\n",
            "[38400/60000 ( 64%)]  Loss: 0.0789\n",
            "[51200/60000 ( 85%)]  Loss: 0.1122\n",
            "\n",
            "Average test loss: 0.1155  Accuracy: 9634/10000 (96.34%)\n",
            "\n",
            "Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.0398\n",
            "[12800/60000 ( 21%)]  Loss: 0.0726\n",
            "[25600/60000 ( 43%)]  Loss: 0.0942\n",
            "[38400/60000 ( 64%)]  Loss: 0.0659\n",
            "[51200/60000 ( 85%)]  Loss: 0.0902\n",
            "\n",
            "Average test loss: 0.0955  Accuracy: 9714/10000 (97.14%)\n",
            "\n",
            "Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.1930\n",
            "[12800/60000 ( 21%)]  Loss: 0.1228\n",
            "[25600/60000 ( 43%)]  Loss: 0.0387\n",
            "[38400/60000 ( 64%)]  Loss: 0.0520\n",
            "[51200/60000 ( 85%)]  Loss: 0.1167\n",
            "\n",
            "Average test loss: 0.0828  Accuracy: 9755/10000 (97.55%)\n",
            "\n",
            "Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.0603\n",
            "[12800/60000 ( 21%)]  Loss: 0.0755\n",
            "[25600/60000 ( 43%)]  Loss: 0.0883\n",
            "[38400/60000 ( 64%)]  Loss: 0.1098\n",
            "[51200/60000 ( 85%)]  Loss: 0.2097\n",
            "\n",
            "Average test loss: 0.0977  Accuracy: 9691/10000 (96.91%)\n",
            "\n",
            "Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.0576\n",
            "[12800/60000 ( 21%)]  Loss: 0.0306\n",
            "[25600/60000 ( 43%)]  Loss: 0.0567\n",
            "[38400/60000 ( 64%)]  Loss: 0.0624\n",
            "[51200/60000 ( 85%)]  Loss: 0.0157\n",
            "\n",
            "Average test loss: 0.0803  Accuracy: 9769/10000 (97.69%)\n",
            "\n",
            "Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.0889\n",
            "[12800/60000 ( 21%)]  Loss: 0.0226\n",
            "[25600/60000 ( 43%)]  Loss: 0.0045\n",
            "[38400/60000 ( 64%)]  Loss: 0.0445\n",
            "[51200/60000 ( 85%)]  Loss: 0.0343\n",
            "\n",
            "Average test loss: 0.0777  Accuracy: 9763/10000 (97.63%)\n",
            "\n",
            "Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.0243\n",
            "[12800/60000 ( 21%)]  Loss: 0.0103\n",
            "[25600/60000 ( 43%)]  Loss: 0.0120\n",
            "[38400/60000 ( 64%)]  Loss: 0.0571\n",
            "[51200/60000 ( 85%)]  Loss: 0.0421\n",
            "\n",
            "Average test loss: 0.0831  Accuracy: 9751/10000 (97.51%)\n",
            "\n",
            "Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0304\n",
            "[12800/60000 ( 21%)]  Loss: 0.0509\n",
            "[25600/60000 ( 43%)]  Loss: 0.0080\n",
            "[38400/60000 ( 64%)]  Loss: 0.0225\n",
            "[51200/60000 ( 85%)]  Loss: 0.0337\n",
            "\n",
            "Average test loss: 0.0604  Accuracy: 9820/10000 (98.20%)\n",
            "\n",
            "Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0096\n",
            "[12800/60000 ( 21%)]  Loss: 0.0077\n",
            "[25600/60000 ( 43%)]  Loss: 0.0659\n",
            "[38400/60000 ( 64%)]  Loss: 0.0454\n",
            "[51200/60000 ( 85%)]  Loss: 0.0543\n",
            "\n",
            "Average test loss: 0.0717  Accuracy: 9811/10000 (98.11%)\n",
            "\n",
            "Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.0508\n",
            "[12800/60000 ( 21%)]  Loss: 0.0460\n",
            "[25600/60000 ( 43%)]  Loss: 0.0217\n",
            "[38400/60000 ( 64%)]  Loss: 0.0682\n",
            "[51200/60000 ( 85%)]  Loss: 0.0055\n",
            "\n",
            "Average test loss: 0.0558  Accuracy: 9841/10000 (98.41%)\n",
            "\n",
            "Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0049\n",
            "[12800/60000 ( 21%)]  Loss: 0.0867\n",
            "[25600/60000 ( 43%)]  Loss: 0.0190\n",
            "[38400/60000 ( 64%)]  Loss: 0.0082\n",
            "[51200/60000 ( 85%)]  Loss: 0.0081\n",
            "\n",
            "Average test loss: 0.0789  Accuracy: 9794/10000 (97.94%)\n",
            "\n",
            "Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0053\n",
            "[12800/60000 ( 21%)]  Loss: 0.0025\n",
            "[25600/60000 ( 43%)]  Loss: 0.0123\n",
            "[38400/60000 ( 64%)]  Loss: 0.0089\n",
            "[51200/60000 ( 85%)]  Loss: 0.0119\n",
            "\n",
            "Average test loss: 0.0763  Accuracy: 9809/10000 (98.09%)\n",
            "\n",
            "Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0448\n",
            "[12800/60000 ( 21%)]  Loss: 0.0521\n",
            "[25600/60000 ( 43%)]  Loss: 0.0122\n",
            "[38400/60000 ( 64%)]  Loss: 0.0584\n",
            "[51200/60000 ( 85%)]  Loss: 0.0045\n",
            "\n",
            "Average test loss: 0.0558  Accuracy: 9865/10000 (98.65%)\n",
            "\n",
            "Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0087\n",
            "[12800/60000 ( 21%)]  Loss: 0.1043\n",
            "[25600/60000 ( 43%)]  Loss: 0.0175\n",
            "[38400/60000 ( 64%)]  Loss: 0.0099\n",
            "[51200/60000 ( 85%)]  Loss: 0.0175\n",
            "\n",
            "Average test loss: 0.0557  Accuracy: 9862/10000 (98.62%)\n",
            "\n",
            "Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0067\n",
            "[12800/60000 ( 21%)]  Loss: 0.0036\n",
            "[25600/60000 ( 43%)]  Loss: 0.0192\n",
            "[38400/60000 ( 64%)]  Loss: 0.0033\n",
            "[51200/60000 ( 85%)]  Loss: 0.0002\n",
            "\n",
            "Average test loss: 0.0614  Accuracy: 9855/10000 (98.55%)\n",
            "\n",
            "Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0038\n",
            "[12800/60000 ( 21%)]  Loss: 0.0031\n",
            "[25600/60000 ( 43%)]  Loss: 0.0048\n",
            "[38400/60000 ( 64%)]  Loss: 0.0008\n",
            "[51200/60000 ( 85%)]  Loss: 0.0020\n",
            "\n",
            "Average test loss: 0.0580  Accuracy: 9855/10000 (98.55%)\n",
            "\n",
            "Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0022\n",
            "[12800/60000 ( 21%)]  Loss: 0.0129\n",
            "[25600/60000 ( 43%)]  Loss: 0.0065\n",
            "[38400/60000 ( 64%)]  Loss: 0.0048\n",
            "[51200/60000 ( 85%)]  Loss: 0.0034\n",
            "\n",
            "Average test loss: 0.0832  Accuracy: 9798/10000 (97.98%)\n",
            "\n",
            "Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0001\n",
            "[12800/60000 ( 21%)]  Loss: 0.0018\n",
            "[25600/60000 ( 43%)]  Loss: 0.0268\n",
            "[38400/60000 ( 64%)]  Loss: 0.0783\n",
            "[51200/60000 ( 85%)]  Loss: 0.0004\n",
            "\n",
            "Average test loss: 0.0796  Accuracy: 9845/10000 (98.45%)\n",
            "\n",
            "Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0008\n",
            "[12800/60000 ( 21%)]  Loss: 0.0027\n",
            "[25600/60000 ( 43%)]  Loss: 0.0189\n",
            "[38400/60000 ( 64%)]  Loss: 0.0000\n",
            "[51200/60000 ( 85%)]  Loss: 0.0123\n",
            "\n",
            "Average test loss: 0.0705  Accuracy: 9859/10000 (98.59%)\n",
            "\n",
            "Execution time: 730.28 seconds\n",
            "-----------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------\n",
            "Running experiment with dim=64, depth=4, heads=8, mlp_dim=512\n",
            "Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.3222\n",
            "[12800/60000 ( 21%)]  Loss: 0.9312\n",
            "[25600/60000 ( 43%)]  Loss: 0.4232\n",
            "[38400/60000 ( 64%)]  Loss: 0.3283\n",
            "[51200/60000 ( 85%)]  Loss: 0.3273\n",
            "\n",
            "Average test loss: 0.2770  Accuracy: 9128/10000 (91.28%)\n",
            "\n",
            "Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.2769\n",
            "[12800/60000 ( 21%)]  Loss: 0.3870\n",
            "[25600/60000 ( 43%)]  Loss: 0.0931\n",
            "[38400/60000 ( 64%)]  Loss: 0.1472\n",
            "[51200/60000 ( 85%)]  Loss: 0.1140\n",
            "\n",
            "Average test loss: 0.1332  Accuracy: 9572/10000 (95.72%)\n",
            "\n",
            "Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.0896\n",
            "[12800/60000 ( 21%)]  Loss: 0.1435\n",
            "[25600/60000 ( 43%)]  Loss: 0.2451\n",
            "[38400/60000 ( 64%)]  Loss: 0.0389\n",
            "[51200/60000 ( 85%)]  Loss: 0.0643\n",
            "\n",
            "Average test loss: 0.1218  Accuracy: 9611/10000 (96.11%)\n",
            "\n",
            "Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.1397\n",
            "[12800/60000 ( 21%)]  Loss: 0.0543\n",
            "[25600/60000 ( 43%)]  Loss: 0.0416\n",
            "[38400/60000 ( 64%)]  Loss: 0.1649\n",
            "[51200/60000 ( 85%)]  Loss: 0.0503\n",
            "\n",
            "Average test loss: 0.1145  Accuracy: 9633/10000 (96.33%)\n",
            "\n",
            "Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.1980\n",
            "[12800/60000 ( 21%)]  Loss: 0.1484\n",
            "[25600/60000 ( 43%)]  Loss: 0.1118\n",
            "[38400/60000 ( 64%)]  Loss: 0.0805\n",
            "[51200/60000 ( 85%)]  Loss: 0.0506\n",
            "\n",
            "Average test loss: 0.0795  Accuracy: 9754/10000 (97.54%)\n",
            "\n",
            "Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.0810\n",
            "[12800/60000 ( 21%)]  Loss: 0.0807\n",
            "[25600/60000 ( 43%)]  Loss: 0.1211\n",
            "[38400/60000 ( 64%)]  Loss: 0.0273\n",
            "[51200/60000 ( 85%)]  Loss: 0.0072\n",
            "\n",
            "Average test loss: 0.0863  Accuracy: 9740/10000 (97.40%)\n",
            "\n",
            "Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.0227\n",
            "[12800/60000 ( 21%)]  Loss: 0.0546\n",
            "[25600/60000 ( 43%)]  Loss: 0.0586\n",
            "[38400/60000 ( 64%)]  Loss: 0.0286\n",
            "[51200/60000 ( 85%)]  Loss: 0.0750\n",
            "\n",
            "Average test loss: 0.0784  Accuracy: 9765/10000 (97.65%)\n",
            "\n",
            "Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.0336\n",
            "[12800/60000 ( 21%)]  Loss: 0.0610\n",
            "[25600/60000 ( 43%)]  Loss: 0.0562\n",
            "[38400/60000 ( 64%)]  Loss: 0.0403\n",
            "[51200/60000 ( 85%)]  Loss: 0.0292\n",
            "\n",
            "Average test loss: 0.0685  Accuracy: 9795/10000 (97.95%)\n",
            "\n",
            "Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0370\n",
            "[12800/60000 ( 21%)]  Loss: 0.0118\n",
            "[25600/60000 ( 43%)]  Loss: 0.0425\n",
            "[38400/60000 ( 64%)]  Loss: 0.0329\n",
            "[51200/60000 ( 85%)]  Loss: 0.0486\n",
            "\n",
            "Average test loss: 0.0686  Accuracy: 9794/10000 (97.94%)\n",
            "\n",
            "Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0607\n",
            "[12800/60000 ( 21%)]  Loss: 0.0342\n",
            "[25600/60000 ( 43%)]  Loss: 0.0475\n",
            "[38400/60000 ( 64%)]  Loss: 0.0255\n",
            "[51200/60000 ( 85%)]  Loss: 0.0307\n",
            "\n",
            "Average test loss: 0.0828  Accuracy: 9763/10000 (97.63%)\n",
            "\n",
            "Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.0406\n",
            "[12800/60000 ( 21%)]  Loss: 0.0173\n",
            "[25600/60000 ( 43%)]  Loss: 0.0038\n",
            "[38400/60000 ( 64%)]  Loss: 0.0664\n",
            "[51200/60000 ( 85%)]  Loss: 0.0332\n",
            "\n",
            "Average test loss: 0.0664  Accuracy: 9816/10000 (98.16%)\n",
            "\n",
            "Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0745\n",
            "[12800/60000 ( 21%)]  Loss: 0.0041\n",
            "[25600/60000 ( 43%)]  Loss: 0.0113\n",
            "[38400/60000 ( 64%)]  Loss: 0.0391\n",
            "[51200/60000 ( 85%)]  Loss: 0.0168\n",
            "\n",
            "Average test loss: 0.0717  Accuracy: 9795/10000 (97.95%)\n",
            "\n",
            "Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0566\n",
            "[12800/60000 ( 21%)]  Loss: 0.0051\n",
            "[25600/60000 ( 43%)]  Loss: 0.0114\n",
            "[38400/60000 ( 64%)]  Loss: 0.0135\n",
            "[51200/60000 ( 85%)]  Loss: 0.0025\n",
            "\n",
            "Average test loss: 0.0644  Accuracy: 9814/10000 (98.14%)\n",
            "\n",
            "Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0158\n",
            "[12800/60000 ( 21%)]  Loss: 0.0121\n",
            "[25600/60000 ( 43%)]  Loss: 0.0044\n",
            "[38400/60000 ( 64%)]  Loss: 0.0168\n",
            "[51200/60000 ( 85%)]  Loss: 0.0622\n",
            "\n",
            "Average test loss: 0.0699  Accuracy: 9806/10000 (98.06%)\n",
            "\n",
            "Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0092\n",
            "[12800/60000 ( 21%)]  Loss: 0.0114\n",
            "[25600/60000 ( 43%)]  Loss: 0.0044\n",
            "[38400/60000 ( 64%)]  Loss: 0.0310\n",
            "[51200/60000 ( 85%)]  Loss: 0.0212\n",
            "\n",
            "Average test loss: 0.0666  Accuracy: 9828/10000 (98.28%)\n",
            "\n",
            "Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0026\n",
            "[12800/60000 ( 21%)]  Loss: 0.0208\n",
            "[25600/60000 ( 43%)]  Loss: 0.0065\n",
            "[38400/60000 ( 64%)]  Loss: 0.0224\n",
            "[51200/60000 ( 85%)]  Loss: 0.0173\n",
            "\n",
            "Average test loss: 0.0669  Accuracy: 9832/10000 (98.32%)\n",
            "\n",
            "Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0020\n",
            "[12800/60000 ( 21%)]  Loss: 0.0532\n",
            "[25600/60000 ( 43%)]  Loss: 0.0021\n",
            "[38400/60000 ( 64%)]  Loss: 0.0327\n",
            "[51200/60000 ( 85%)]  Loss: 0.0656\n",
            "\n",
            "Average test loss: 0.0675  Accuracy: 9831/10000 (98.31%)\n",
            "\n",
            "Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0016\n",
            "[12800/60000 ( 21%)]  Loss: 0.0171\n",
            "[25600/60000 ( 43%)]  Loss: 0.0558\n",
            "[38400/60000 ( 64%)]  Loss: 0.0114\n",
            "[51200/60000 ( 85%)]  Loss: 0.0137\n",
            "\n",
            "Average test loss: 0.0737  Accuracy: 9817/10000 (98.17%)\n",
            "\n",
            "Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0027\n",
            "[12800/60000 ( 21%)]  Loss: 0.0019\n",
            "[25600/60000 ( 43%)]  Loss: 0.0145\n",
            "[38400/60000 ( 64%)]  Loss: 0.0020\n",
            "[51200/60000 ( 85%)]  Loss: 0.0022\n",
            "\n",
            "Average test loss: 0.0654  Accuracy: 9840/10000 (98.40%)\n",
            "\n",
            "Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0056\n",
            "[12800/60000 ( 21%)]  Loss: 0.0022\n",
            "[25600/60000 ( 43%)]  Loss: 0.0004\n",
            "[38400/60000 ( 64%)]  Loss: 0.0068\n",
            "[51200/60000 ( 85%)]  Loss: 0.0289\n",
            "\n",
            "Average test loss: 0.0846  Accuracy: 9807/10000 (98.07%)\n",
            "\n",
            "Execution time: 358.50 seconds\n",
            "-----------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------\n",
            "Running experiment with dim=256, depth=8, heads=8, mlp_dim=64\n",
            "Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.3316\n",
            "[12800/60000 ( 21%)]  Loss: 0.6435\n",
            "[25600/60000 ( 43%)]  Loss: 0.2155\n",
            "[38400/60000 ( 64%)]  Loss: 0.2426\n",
            "[51200/60000 ( 85%)]  Loss: 0.1703\n",
            "\n",
            "Average test loss: 0.1452  Accuracy: 9542/10000 (95.42%)\n",
            "\n",
            "Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.0687\n",
            "[12800/60000 ( 21%)]  Loss: 0.1211\n",
            "[25600/60000 ( 43%)]  Loss: 0.1630\n",
            "[38400/60000 ( 64%)]  Loss: 0.2390\n",
            "[51200/60000 ( 85%)]  Loss: 0.0582\n",
            "\n",
            "Average test loss: 0.1176  Accuracy: 9651/10000 (96.51%)\n",
            "\n",
            "Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.0709\n",
            "[12800/60000 ( 21%)]  Loss: 0.1067\n",
            "[25600/60000 ( 43%)]  Loss: 0.1117\n",
            "[38400/60000 ( 64%)]  Loss: 0.1675\n",
            "[51200/60000 ( 85%)]  Loss: 0.1310\n",
            "\n",
            "Average test loss: 0.1150  Accuracy: 9640/10000 (96.40%)\n",
            "\n",
            "Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.0690\n",
            "[12800/60000 ( 21%)]  Loss: 0.1346\n",
            "[25600/60000 ( 43%)]  Loss: 0.1316\n",
            "[38400/60000 ( 64%)]  Loss: 0.0894\n",
            "[51200/60000 ( 85%)]  Loss: 0.0387\n",
            "\n",
            "Average test loss: 0.0866  Accuracy: 9730/10000 (97.30%)\n",
            "\n",
            "Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.0611\n",
            "[12800/60000 ( 21%)]  Loss: 0.1424\n",
            "[25600/60000 ( 43%)]  Loss: 0.1918\n",
            "[38400/60000 ( 64%)]  Loss: 0.0148\n",
            "[51200/60000 ( 85%)]  Loss: 0.0675\n",
            "\n",
            "Average test loss: 0.0782  Accuracy: 9763/10000 (97.63%)\n",
            "\n",
            "Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.0249\n",
            "[12800/60000 ( 21%)]  Loss: 0.0492\n",
            "[25600/60000 ( 43%)]  Loss: 0.0186\n",
            "[38400/60000 ( 64%)]  Loss: 0.0813\n",
            "[51200/60000 ( 85%)]  Loss: 0.1120\n",
            "\n",
            "Average test loss: 0.0826  Accuracy: 9740/10000 (97.40%)\n",
            "\n",
            "Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.0912\n",
            "[12800/60000 ( 21%)]  Loss: 0.0251\n",
            "[25600/60000 ( 43%)]  Loss: 0.0272\n",
            "[38400/60000 ( 64%)]  Loss: 0.0266\n",
            "[51200/60000 ( 85%)]  Loss: 0.0441\n",
            "\n",
            "Average test loss: 0.0751  Accuracy: 9768/10000 (97.68%)\n",
            "\n",
            "Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.0394\n",
            "[12800/60000 ( 21%)]  Loss: 0.0582\n",
            "[25600/60000 ( 43%)]  Loss: 0.0058\n",
            "[38400/60000 ( 64%)]  Loss: 0.0422\n",
            "[51200/60000 ( 85%)]  Loss: 0.0129\n",
            "\n",
            "Average test loss: 0.0652  Accuracy: 9800/10000 (98.00%)\n",
            "\n",
            "Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0233\n",
            "[12800/60000 ( 21%)]  Loss: 0.0775\n",
            "[25600/60000 ( 43%)]  Loss: 0.0056\n",
            "[38400/60000 ( 64%)]  Loss: 0.0710\n",
            "[51200/60000 ( 85%)]  Loss: 0.0231\n",
            "\n",
            "Average test loss: 0.0830  Accuracy: 9758/10000 (97.58%)\n",
            "\n",
            "Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0183\n",
            "[12800/60000 ( 21%)]  Loss: 0.0144\n",
            "[25600/60000 ( 43%)]  Loss: 0.0376\n",
            "[38400/60000 ( 64%)]  Loss: 0.0365\n",
            "[51200/60000 ( 85%)]  Loss: 0.0680\n",
            "\n",
            "Average test loss: 0.0828  Accuracy: 9748/10000 (97.48%)\n",
            "\n",
            "Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.1289\n",
            "[12800/60000 ( 21%)]  Loss: 0.0304\n",
            "[25600/60000 ( 43%)]  Loss: 0.0204\n",
            "[38400/60000 ( 64%)]  Loss: 0.0113\n",
            "[51200/60000 ( 85%)]  Loss: 0.0070\n",
            "\n",
            "Average test loss: 0.0585  Accuracy: 9835/10000 (98.35%)\n",
            "\n",
            "Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0530\n",
            "[12800/60000 ( 21%)]  Loss: 0.0385\n",
            "[25600/60000 ( 43%)]  Loss: 0.0173\n",
            "[38400/60000 ( 64%)]  Loss: 0.0143\n",
            "[51200/60000 ( 85%)]  Loss: 0.0698\n",
            "\n",
            "Average test loss: 0.0625  Accuracy: 9812/10000 (98.12%)\n",
            "\n",
            "Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0068\n",
            "[12800/60000 ( 21%)]  Loss: 0.0155\n",
            "[25600/60000 ( 43%)]  Loss: 0.0183\n",
            "[38400/60000 ( 64%)]  Loss: 0.0088\n",
            "[51200/60000 ( 85%)]  Loss: 0.0102\n",
            "\n",
            "Average test loss: 0.0754  Accuracy: 9801/10000 (98.01%)\n",
            "\n",
            "Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0226\n",
            "[12800/60000 ( 21%)]  Loss: 0.0064\n",
            "[25600/60000 ( 43%)]  Loss: 0.0442\n",
            "[38400/60000 ( 64%)]  Loss: 0.0046\n",
            "[51200/60000 ( 85%)]  Loss: 0.0395\n",
            "\n",
            "Average test loss: 0.0745  Accuracy: 9805/10000 (98.05%)\n",
            "\n",
            "Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0201\n",
            "[12800/60000 ( 21%)]  Loss: 0.0049\n",
            "[25600/60000 ( 43%)]  Loss: 0.0623\n",
            "[38400/60000 ( 64%)]  Loss: 0.0352\n",
            "[51200/60000 ( 85%)]  Loss: 0.0056\n",
            "\n",
            "Average test loss: 0.0745  Accuracy: 9823/10000 (98.23%)\n",
            "\n",
            "Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0184\n",
            "[12800/60000 ( 21%)]  Loss: 0.0039\n",
            "[25600/60000 ( 43%)]  Loss: 0.0011\n",
            "[38400/60000 ( 64%)]  Loss: 0.0012\n",
            "[51200/60000 ( 85%)]  Loss: 0.0222\n",
            "\n",
            "Average test loss: 0.0641  Accuracy: 9850/10000 (98.50%)\n",
            "\n",
            "Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0040\n",
            "[12800/60000 ( 21%)]  Loss: 0.0099\n",
            "[25600/60000 ( 43%)]  Loss: 0.0004\n",
            "[38400/60000 ( 64%)]  Loss: 0.0023\n",
            "[51200/60000 ( 85%)]  Loss: 0.0012\n",
            "\n",
            "Average test loss: 0.0649  Accuracy: 9839/10000 (98.39%)\n",
            "\n",
            "Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0366\n",
            "[12800/60000 ( 21%)]  Loss: 0.0079\n",
            "[25600/60000 ( 43%)]  Loss: 0.0911\n",
            "[38400/60000 ( 64%)]  Loss: 0.0395\n",
            "[51200/60000 ( 85%)]  Loss: 0.0040\n",
            "\n",
            "Average test loss: 0.0694  Accuracy: 9827/10000 (98.27%)\n",
            "\n",
            "Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0094\n",
            "[12800/60000 ( 21%)]  Loss: 0.0016\n",
            "[25600/60000 ( 43%)]  Loss: 0.0080\n",
            "[38400/60000 ( 64%)]  Loss: 0.0062\n",
            "[51200/60000 ( 85%)]  Loss: 0.0007\n",
            "\n",
            "Average test loss: 0.0662  Accuracy: 9836/10000 (98.36%)\n",
            "\n",
            "Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0004\n",
            "[12800/60000 ( 21%)]  Loss: 0.0005\n",
            "[25600/60000 ( 43%)]  Loss: 0.0065\n",
            "[38400/60000 ( 64%)]  Loss: 0.0008\n",
            "[51200/60000 ( 85%)]  Loss: 0.0084\n",
            "\n",
            "Average test loss: 0.0703  Accuracy: 9839/10000 (98.39%)\n",
            "\n",
            "Execution time: 999.32 seconds\n",
            "-----------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------\n",
            "Running experiment with dim=256, depth=10, heads=8, mlp_dim=128\n",
            "Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.3723\n",
            "[12800/60000 ( 21%)]  Loss: 0.5808\n",
            "[25600/60000 ( 43%)]  Loss: 0.2434\n",
            "[38400/60000 ( 64%)]  Loss: 0.1480\n",
            "[51200/60000 ( 85%)]  Loss: 0.0977\n",
            "\n",
            "Average test loss: 0.1676  Accuracy: 9453/10000 (94.53%)\n",
            "\n",
            "Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.1392\n",
            "[12800/60000 ( 21%)]  Loss: 0.1086\n",
            "[25600/60000 ( 43%)]  Loss: 0.2036\n",
            "[38400/60000 ( 64%)]  Loss: 0.1090\n",
            "[51200/60000 ( 85%)]  Loss: 0.0675\n",
            "\n",
            "Average test loss: 0.1054  Accuracy: 9684/10000 (96.84%)\n",
            "\n",
            "Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.0988\n",
            "[12800/60000 ( 21%)]  Loss: 0.1314\n",
            "[25600/60000 ( 43%)]  Loss: 0.0489\n",
            "[38400/60000 ( 64%)]  Loss: 0.0713\n",
            "[51200/60000 ( 85%)]  Loss: 0.0901\n",
            "\n",
            "Average test loss: 0.0801  Accuracy: 9750/10000 (97.50%)\n",
            "\n",
            "Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.0880\n",
            "[12800/60000 ( 21%)]  Loss: 0.0216\n",
            "[25600/60000 ( 43%)]  Loss: 0.0783\n",
            "[38400/60000 ( 64%)]  Loss: 0.1473\n",
            "[51200/60000 ( 85%)]  Loss: 0.0653\n",
            "\n",
            "Average test loss: 0.0927  Accuracy: 9722/10000 (97.22%)\n",
            "\n",
            "Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.0933\n",
            "[12800/60000 ( 21%)]  Loss: 0.0269\n",
            "[25600/60000 ( 43%)]  Loss: 0.1779\n",
            "[38400/60000 ( 64%)]  Loss: 0.0529\n",
            "[51200/60000 ( 85%)]  Loss: 0.0891\n",
            "\n",
            "Average test loss: 0.0770  Accuracy: 9758/10000 (97.58%)\n",
            "\n",
            "Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.0474\n",
            "[12800/60000 ( 21%)]  Loss: 0.0597\n",
            "[25600/60000 ( 43%)]  Loss: 0.0421\n",
            "[38400/60000 ( 64%)]  Loss: 0.0832\n",
            "[51200/60000 ( 85%)]  Loss: 0.0355\n",
            "\n",
            "Average test loss: 0.1112  Accuracy: 9675/10000 (96.75%)\n",
            "\n",
            "Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.0826\n",
            "[12800/60000 ( 21%)]  Loss: 0.0595\n",
            "[25600/60000 ( 43%)]  Loss: 0.0226\n",
            "[38400/60000 ( 64%)]  Loss: 0.0432\n",
            "[51200/60000 ( 85%)]  Loss: 0.0215\n",
            "\n",
            "Average test loss: 0.0799  Accuracy: 9786/10000 (97.86%)\n",
            "\n",
            "Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.1399\n",
            "[12800/60000 ( 21%)]  Loss: 0.0313\n",
            "[25600/60000 ( 43%)]  Loss: 0.0839\n",
            "[38400/60000 ( 64%)]  Loss: 0.0126\n",
            "[51200/60000 ( 85%)]  Loss: 0.0294\n",
            "\n",
            "Average test loss: 0.0750  Accuracy: 9775/10000 (97.75%)\n",
            "\n",
            "Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0967\n",
            "[12800/60000 ( 21%)]  Loss: 0.0080\n",
            "[25600/60000 ( 43%)]  Loss: 0.0135\n",
            "[38400/60000 ( 64%)]  Loss: 0.0271\n",
            "[51200/60000 ( 85%)]  Loss: 0.0301\n",
            "\n",
            "Average test loss: 0.0720  Accuracy: 9792/10000 (97.92%)\n",
            "\n",
            "Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0310\n",
            "[12800/60000 ( 21%)]  Loss: 0.0289\n",
            "[25600/60000 ( 43%)]  Loss: 0.0477\n",
            "[38400/60000 ( 64%)]  Loss: 0.0290\n",
            "[51200/60000 ( 85%)]  Loss: 0.0163\n",
            "\n",
            "Average test loss: 0.0617  Accuracy: 9815/10000 (98.15%)\n",
            "\n",
            "Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.0677\n",
            "[12800/60000 ( 21%)]  Loss: 0.0037\n",
            "[25600/60000 ( 43%)]  Loss: 0.0429\n",
            "[38400/60000 ( 64%)]  Loss: 0.0860\n",
            "[51200/60000 ( 85%)]  Loss: 0.0463\n",
            "\n",
            "Average test loss: 0.0686  Accuracy: 9805/10000 (98.05%)\n",
            "\n",
            "Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0086\n",
            "[12800/60000 ( 21%)]  Loss: 0.0020\n",
            "[25600/60000 ( 43%)]  Loss: 0.0444\n",
            "[38400/60000 ( 64%)]  Loss: 0.0367\n",
            "[51200/60000 ( 85%)]  Loss: 0.0097\n",
            "\n",
            "Average test loss: 0.0591  Accuracy: 9833/10000 (98.33%)\n",
            "\n",
            "Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0070\n",
            "[12800/60000 ( 21%)]  Loss: 0.0210\n",
            "[25600/60000 ( 43%)]  Loss: 0.0291\n",
            "[38400/60000 ( 64%)]  Loss: 0.0254\n",
            "[51200/60000 ( 85%)]  Loss: 0.0108\n",
            "\n",
            "Average test loss: 0.0712  Accuracy: 9828/10000 (98.28%)\n",
            "\n",
            "Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0169\n",
            "[12800/60000 ( 21%)]  Loss: 0.0036\n",
            "[25600/60000 ( 43%)]  Loss: 0.0049\n",
            "[38400/60000 ( 64%)]  Loss: 0.0068\n",
            "[51200/60000 ( 85%)]  Loss: 0.0074\n",
            "\n",
            "Average test loss: 0.0720  Accuracy: 9822/10000 (98.22%)\n",
            "\n",
            "Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0002\n",
            "[12800/60000 ( 21%)]  Loss: 0.0034\n",
            "[25600/60000 ( 43%)]  Loss: 0.0066\n",
            "[38400/60000 ( 64%)]  Loss: 0.0093\n",
            "[51200/60000 ( 85%)]  Loss: 0.0239\n",
            "\n",
            "Average test loss: 0.0672  Accuracy: 9821/10000 (98.21%)\n",
            "\n",
            "Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0226\n",
            "[12800/60000 ( 21%)]  Loss: 0.0034\n",
            "[25600/60000 ( 43%)]  Loss: 0.0167\n",
            "[38400/60000 ( 64%)]  Loss: 0.0306\n",
            "[51200/60000 ( 85%)]  Loss: 0.0136\n",
            "\n",
            "Average test loss: 0.0606  Accuracy: 9852/10000 (98.52%)\n",
            "\n",
            "Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0058\n",
            "[12800/60000 ( 21%)]  Loss: 0.0021\n",
            "[25600/60000 ( 43%)]  Loss: 0.0008\n",
            "[38400/60000 ( 64%)]  Loss: 0.0057\n",
            "[51200/60000 ( 85%)]  Loss: 0.0007\n",
            "\n",
            "Average test loss: 0.0695  Accuracy: 9843/10000 (98.43%)\n",
            "\n",
            "Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0041\n",
            "[12800/60000 ( 21%)]  Loss: 0.0017\n",
            "[25600/60000 ( 43%)]  Loss: 0.0009\n",
            "[38400/60000 ( 64%)]  Loss: 0.0008\n",
            "[51200/60000 ( 85%)]  Loss: 0.0233\n",
            "\n",
            "Average test loss: 0.0615  Accuracy: 9856/10000 (98.56%)\n",
            "\n",
            "Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0109\n",
            "[12800/60000 ( 21%)]  Loss: 0.0038\n",
            "[25600/60000 ( 43%)]  Loss: 0.0035\n",
            "[38400/60000 ( 64%)]  Loss: 0.0010\n",
            "[51200/60000 ( 85%)]  Loss: 0.0018\n",
            "\n",
            "Average test loss: 0.0742  Accuracy: 9834/10000 (98.34%)\n",
            "\n",
            "Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0007\n",
            "[12800/60000 ( 21%)]  Loss: 0.0126\n",
            "[25600/60000 ( 43%)]  Loss: 0.0014\n",
            "[38400/60000 ( 64%)]  Loss: 0.0191\n",
            "[51200/60000 ( 85%)]  Loss: 0.0022\n",
            "\n",
            "Average test loss: 0.0624  Accuracy: 9831/10000 (98.31%)\n",
            "\n",
            "Execution time: 1302.01 seconds\n",
            "-----------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------\n",
            "Running experiment with dim=32, depth=10, heads=8, mlp_dim=256\n",
            "Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.3573\n",
            "[12800/60000 ( 21%)]  Loss: 1.3123\n",
            "[25600/60000 ( 43%)]  Loss: 0.7751\n",
            "[38400/60000 ( 64%)]  Loss: 0.4498\n",
            "[51200/60000 ( 85%)]  Loss: 0.2707\n",
            "\n",
            "Average test loss: 0.3135  Accuracy: 8994/10000 (89.94%)\n",
            "\n",
            "Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.2820\n",
            "[12800/60000 ( 21%)]  Loss: 0.2389\n",
            "[25600/60000 ( 43%)]  Loss: 0.3451\n",
            "[38400/60000 ( 64%)]  Loss: 0.3715\n",
            "[51200/60000 ( 85%)]  Loss: 0.1815\n",
            "\n",
            "Average test loss: 0.1618  Accuracy: 9493/10000 (94.93%)\n",
            "\n",
            "Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.1418\n",
            "[12800/60000 ( 21%)]  Loss: 0.1706\n",
            "[25600/60000 ( 43%)]  Loss: 0.1690\n",
            "[38400/60000 ( 64%)]  Loss: 0.2632\n",
            "[51200/60000 ( 85%)]  Loss: 0.0800\n",
            "\n",
            "Average test loss: 0.1287  Accuracy: 9588/10000 (95.88%)\n",
            "\n",
            "Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.2046\n",
            "[12800/60000 ( 21%)]  Loss: 0.1053\n",
            "[25600/60000 ( 43%)]  Loss: 0.0871\n",
            "[38400/60000 ( 64%)]  Loss: 0.2212\n",
            "[51200/60000 ( 85%)]  Loss: 0.0823\n",
            "\n",
            "Average test loss: 0.1004  Accuracy: 9681/10000 (96.81%)\n",
            "\n",
            "Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.1456\n",
            "[12800/60000 ( 21%)]  Loss: 0.0654\n",
            "[25600/60000 ( 43%)]  Loss: 0.1349\n",
            "[38400/60000 ( 64%)]  Loss: 0.1238\n",
            "[51200/60000 ( 85%)]  Loss: 0.0842\n",
            "\n",
            "Average test loss: 0.0988  Accuracy: 9699/10000 (96.99%)\n",
            "\n",
            "Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.0901\n",
            "[12800/60000 ( 21%)]  Loss: 0.0940\n",
            "[25600/60000 ( 43%)]  Loss: 0.1375\n",
            "[38400/60000 ( 64%)]  Loss: 0.0434\n",
            "[51200/60000 ( 85%)]  Loss: 0.0411\n",
            "\n",
            "Average test loss: 0.0881  Accuracy: 9717/10000 (97.17%)\n",
            "\n",
            "Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.0615\n",
            "[12800/60000 ( 21%)]  Loss: 0.0352\n",
            "[25600/60000 ( 43%)]  Loss: 0.0917\n",
            "[38400/60000 ( 64%)]  Loss: 0.0573\n",
            "[51200/60000 ( 85%)]  Loss: 0.0906\n",
            "\n",
            "Average test loss: 0.0893  Accuracy: 9714/10000 (97.14%)\n",
            "\n",
            "Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.0326\n",
            "[12800/60000 ( 21%)]  Loss: 0.0627\n",
            "[25600/60000 ( 43%)]  Loss: 0.1283\n",
            "[38400/60000 ( 64%)]  Loss: 0.0336\n",
            "[51200/60000 ( 85%)]  Loss: 0.1792\n",
            "\n",
            "Average test loss: 0.0794  Accuracy: 9750/10000 (97.50%)\n",
            "\n",
            "Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0352\n",
            "[12800/60000 ( 21%)]  Loss: 0.0556\n",
            "[25600/60000 ( 43%)]  Loss: 0.0735\n",
            "[38400/60000 ( 64%)]  Loss: 0.1383\n",
            "[51200/60000 ( 85%)]  Loss: 0.0539\n",
            "\n",
            "Average test loss: 0.0705  Accuracy: 9789/10000 (97.89%)\n",
            "\n",
            "Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0139\n",
            "[12800/60000 ( 21%)]  Loss: 0.0251\n",
            "[25600/60000 ( 43%)]  Loss: 0.0288\n",
            "[38400/60000 ( 64%)]  Loss: 0.0965\n",
            "[51200/60000 ( 85%)]  Loss: 0.0511\n",
            "\n",
            "Average test loss: 0.0742  Accuracy: 9753/10000 (97.53%)\n",
            "\n",
            "Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.0480\n",
            "[12800/60000 ( 21%)]  Loss: 0.1363\n",
            "[25600/60000 ( 43%)]  Loss: 0.0728\n",
            "[38400/60000 ( 64%)]  Loss: 0.0379\n",
            "[51200/60000 ( 85%)]  Loss: 0.1899\n",
            "\n",
            "Average test loss: 0.0804  Accuracy: 9746/10000 (97.46%)\n",
            "\n",
            "Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.1023\n",
            "[12800/60000 ( 21%)]  Loss: 0.0049\n",
            "[25600/60000 ( 43%)]  Loss: 0.0096\n",
            "[38400/60000 ( 64%)]  Loss: 0.0128\n",
            "[51200/60000 ( 85%)]  Loss: 0.0699\n",
            "\n",
            "Average test loss: 0.0647  Accuracy: 9821/10000 (98.21%)\n",
            "\n",
            "Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0279\n",
            "[12800/60000 ( 21%)]  Loss: 0.0595\n",
            "[25600/60000 ( 43%)]  Loss: 0.0979\n",
            "[38400/60000 ( 64%)]  Loss: 0.0726\n",
            "[51200/60000 ( 85%)]  Loss: 0.0225\n",
            "\n",
            "Average test loss: 0.0830  Accuracy: 9747/10000 (97.47%)\n",
            "\n",
            "Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0940\n",
            "[12800/60000 ( 21%)]  Loss: 0.0173\n",
            "[25600/60000 ( 43%)]  Loss: 0.0174\n",
            "[38400/60000 ( 64%)]  Loss: 0.0258\n",
            "[51200/60000 ( 85%)]  Loss: 0.0208\n",
            "\n",
            "Average test loss: 0.0743  Accuracy: 9775/10000 (97.75%)\n",
            "\n",
            "Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0258\n",
            "[12800/60000 ( 21%)]  Loss: 0.0079\n",
            "[25600/60000 ( 43%)]  Loss: 0.0093\n",
            "[38400/60000 ( 64%)]  Loss: 0.0626\n",
            "[51200/60000 ( 85%)]  Loss: 0.0134\n",
            "\n",
            "Average test loss: 0.0756  Accuracy: 9792/10000 (97.92%)\n",
            "\n",
            "Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0026\n",
            "[12800/60000 ( 21%)]  Loss: 0.0295\n",
            "[25600/60000 ( 43%)]  Loss: 0.0140\n",
            "[38400/60000 ( 64%)]  Loss: 0.0211\n",
            "[51200/60000 ( 85%)]  Loss: 0.0093\n",
            "\n",
            "Average test loss: 0.0623  Accuracy: 9821/10000 (98.21%)\n",
            "\n",
            "Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0030\n",
            "[12800/60000 ( 21%)]  Loss: 0.0150\n",
            "[25600/60000 ( 43%)]  Loss: 0.0208\n",
            "[38400/60000 ( 64%)]  Loss: 0.0487\n",
            "[51200/60000 ( 85%)]  Loss: 0.0221\n",
            "\n",
            "Average test loss: 0.0603  Accuracy: 9829/10000 (98.29%)\n",
            "\n",
            "Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0173\n",
            "[12800/60000 ( 21%)]  Loss: 0.0262\n",
            "[25600/60000 ( 43%)]  Loss: 0.0088\n",
            "[38400/60000 ( 64%)]  Loss: 0.0124\n",
            "[51200/60000 ( 85%)]  Loss: 0.0564\n",
            "\n",
            "Average test loss: 0.0741  Accuracy: 9798/10000 (97.98%)\n",
            "\n",
            "Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0108\n",
            "[12800/60000 ( 21%)]  Loss: 0.0217\n",
            "[25600/60000 ( 43%)]  Loss: 0.0150\n",
            "[38400/60000 ( 64%)]  Loss: 0.0080\n",
            "[51200/60000 ( 85%)]  Loss: 0.0213\n",
            "\n",
            "Average test loss: 0.0839  Accuracy: 9795/10000 (97.95%)\n",
            "\n",
            "Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0428\n",
            "[12800/60000 ( 21%)]  Loss: 0.0022\n",
            "[25600/60000 ( 43%)]  Loss: 0.0027\n",
            "[38400/60000 ( 64%)]  Loss: 0.0027\n",
            "[51200/60000 ( 85%)]  Loss: 0.0014\n",
            "\n",
            "Average test loss: 0.0615  Accuracy: 9835/10000 (98.35%)\n",
            "\n",
            "Execution time: 605.68 seconds\n",
            "-----------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------\n",
            "Running experiment with dim=32, depth=8, heads=8, mlp_dim=512\n",
            "Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.3520\n",
            "[12800/60000 ( 21%)]  Loss: 1.6369\n",
            "[25600/60000 ( 43%)]  Loss: 0.9496\n",
            "[38400/60000 ( 64%)]  Loss: 0.7640\n",
            "[51200/60000 ( 85%)]  Loss: 0.4175\n",
            "\n",
            "Average test loss: 0.3208  Accuracy: 8982/10000 (89.82%)\n",
            "\n",
            "Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.3004\n",
            "[12800/60000 ( 21%)]  Loss: 0.4284\n",
            "[25600/60000 ( 43%)]  Loss: 0.3502\n",
            "[38400/60000 ( 64%)]  Loss: 0.1845\n",
            "[51200/60000 ( 85%)]  Loss: 0.2825\n",
            "\n",
            "Average test loss: 0.2004  Accuracy: 9354/10000 (93.54%)\n",
            "\n",
            "Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.2428\n",
            "[12800/60000 ( 21%)]  Loss: 0.2264\n",
            "[25600/60000 ( 43%)]  Loss: 0.1041\n",
            "[38400/60000 ( 64%)]  Loss: 0.1153\n",
            "[51200/60000 ( 85%)]  Loss: 0.1152\n",
            "\n",
            "Average test loss: 0.1463  Accuracy: 9545/10000 (95.45%)\n",
            "\n",
            "Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.1181\n",
            "[12800/60000 ( 21%)]  Loss: 0.1304\n",
            "[25600/60000 ( 43%)]  Loss: 0.2302\n",
            "[38400/60000 ( 64%)]  Loss: 0.1223\n",
            "[51200/60000 ( 85%)]  Loss: 0.1545\n",
            "\n",
            "Average test loss: 0.1259  Accuracy: 9610/10000 (96.10%)\n",
            "\n",
            "Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.0791\n",
            "[12800/60000 ( 21%)]  Loss: 0.1381\n",
            "[25600/60000 ( 43%)]  Loss: 0.1105\n",
            "[38400/60000 ( 64%)]  Loss: 0.1152\n",
            "[51200/60000 ( 85%)]  Loss: 0.1919\n",
            "\n",
            "Average test loss: 0.0991  Accuracy: 9673/10000 (96.73%)\n",
            "\n",
            "Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.0664\n",
            "[12800/60000 ( 21%)]  Loss: 0.0898\n",
            "[25600/60000 ( 43%)]  Loss: 0.1958\n",
            "[38400/60000 ( 64%)]  Loss: 0.1287\n",
            "[51200/60000 ( 85%)]  Loss: 0.1888\n",
            "\n",
            "Average test loss: 0.1038  Accuracy: 9682/10000 (96.82%)\n",
            "\n",
            "Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.0773\n",
            "[12800/60000 ( 21%)]  Loss: 0.0935\n",
            "[25600/60000 ( 43%)]  Loss: 0.0642\n",
            "[38400/60000 ( 64%)]  Loss: 0.1489\n",
            "[51200/60000 ( 85%)]  Loss: 0.0470\n",
            "\n",
            "Average test loss: 0.0793  Accuracy: 9748/10000 (97.48%)\n",
            "\n",
            "Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.0202\n",
            "[12800/60000 ( 21%)]  Loss: 0.0565\n",
            "[25600/60000 ( 43%)]  Loss: 0.0468\n",
            "[38400/60000 ( 64%)]  Loss: 0.1076\n",
            "[51200/60000 ( 85%)]  Loss: 0.0677\n",
            "\n",
            "Average test loss: 0.0859  Accuracy: 9724/10000 (97.24%)\n",
            "\n",
            "Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0369\n",
            "[12800/60000 ( 21%)]  Loss: 0.0246\n",
            "[25600/60000 ( 43%)]  Loss: 0.1271\n",
            "[38400/60000 ( 64%)]  Loss: 0.0396\n",
            "[51200/60000 ( 85%)]  Loss: 0.0089\n",
            "\n",
            "Average test loss: 0.0863  Accuracy: 9726/10000 (97.26%)\n",
            "\n",
            "Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0599\n",
            "[12800/60000 ( 21%)]  Loss: 0.0572\n",
            "[25600/60000 ( 43%)]  Loss: 0.0906\n",
            "[38400/60000 ( 64%)]  Loss: 0.0680\n",
            "[51200/60000 ( 85%)]  Loss: 0.0778\n",
            "\n",
            "Average test loss: 0.0636  Accuracy: 9798/10000 (97.98%)\n",
            "\n",
            "Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.0903\n",
            "[12800/60000 ( 21%)]  Loss: 0.0317\n",
            "[25600/60000 ( 43%)]  Loss: 0.0505\n",
            "[38400/60000 ( 64%)]  Loss: 0.0912\n",
            "[51200/60000 ( 85%)]  Loss: 0.0116\n",
            "\n",
            "Average test loss: 0.0608  Accuracy: 9809/10000 (98.09%)\n",
            "\n",
            "Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0082\n",
            "[12800/60000 ( 21%)]  Loss: 0.0736\n",
            "[25600/60000 ( 43%)]  Loss: 0.0817\n",
            "[38400/60000 ( 64%)]  Loss: 0.0326\n",
            "[51200/60000 ( 85%)]  Loss: 0.0656\n",
            "\n",
            "Average test loss: 0.0667  Accuracy: 9790/10000 (97.90%)\n",
            "\n",
            "Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0336\n",
            "[12800/60000 ( 21%)]  Loss: 0.0425\n",
            "[25600/60000 ( 43%)]  Loss: 0.0103\n",
            "[38400/60000 ( 64%)]  Loss: 0.0342\n",
            "[51200/60000 ( 85%)]  Loss: 0.0308\n",
            "\n",
            "Average test loss: 0.0605  Accuracy: 9803/10000 (98.03%)\n",
            "\n",
            "Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0139\n",
            "[12800/60000 ( 21%)]  Loss: 0.0862\n",
            "[25600/60000 ( 43%)]  Loss: 0.0534\n",
            "[38400/60000 ( 64%)]  Loss: 0.0032\n",
            "[51200/60000 ( 85%)]  Loss: 0.0428\n",
            "\n",
            "Average test loss: 0.0738  Accuracy: 9783/10000 (97.83%)\n",
            "\n",
            "Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0156\n",
            "[12800/60000 ( 21%)]  Loss: 0.0293\n",
            "[25600/60000 ( 43%)]  Loss: 0.0340\n",
            "[38400/60000 ( 64%)]  Loss: 0.0284\n",
            "[51200/60000 ( 85%)]  Loss: 0.0110\n",
            "\n",
            "Average test loss: 0.0730  Accuracy: 9779/10000 (97.79%)\n",
            "\n",
            "Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0097\n",
            "[12800/60000 ( 21%)]  Loss: 0.0223\n",
            "[25600/60000 ( 43%)]  Loss: 0.0245\n",
            "[38400/60000 ( 64%)]  Loss: 0.0372\n",
            "[51200/60000 ( 85%)]  Loss: 0.0508\n",
            "\n",
            "Average test loss: 0.0625  Accuracy: 9823/10000 (98.23%)\n",
            "\n",
            "Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0070\n",
            "[12800/60000 ( 21%)]  Loss: 0.0147\n",
            "[25600/60000 ( 43%)]  Loss: 0.0551\n",
            "[38400/60000 ( 64%)]  Loss: 0.0097\n",
            "[51200/60000 ( 85%)]  Loss: 0.0686\n",
            "\n",
            "Average test loss: 0.0704  Accuracy: 9794/10000 (97.94%)\n",
            "\n",
            "Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0052\n",
            "[12800/60000 ( 21%)]  Loss: 0.0196\n",
            "[25600/60000 ( 43%)]  Loss: 0.0105\n",
            "[38400/60000 ( 64%)]  Loss: 0.0422\n",
            "[51200/60000 ( 85%)]  Loss: 0.0276\n",
            "\n",
            "Average test loss: 0.0728  Accuracy: 9787/10000 (97.87%)\n",
            "\n",
            "Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0067\n",
            "[12800/60000 ( 21%)]  Loss: 0.0091\n",
            "[25600/60000 ( 43%)]  Loss: 0.0089\n",
            "[38400/60000 ( 64%)]  Loss: 0.0586\n",
            "[51200/60000 ( 85%)]  Loss: 0.0343\n",
            "\n",
            "Average test loss: 0.0630  Accuracy: 9809/10000 (98.09%)\n",
            "\n",
            "Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0159\n",
            "[12800/60000 ( 21%)]  Loss: 0.0147\n",
            "[25600/60000 ( 43%)]  Loss: 0.0138\n",
            "[38400/60000 ( 64%)]  Loss: 0.0255\n",
            "[51200/60000 ( 85%)]  Loss: 0.0012\n",
            "\n",
            "Average test loss: 0.0597  Accuracy: 9820/10000 (98.20%)\n",
            "\n",
            "Execution time: 529.60 seconds\n",
            "-----------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------\n",
            "Running experiment with dim=128, depth=4, heads=8, mlp_dim=512\n",
            "Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.3117\n",
            "[12800/60000 ( 21%)]  Loss: 0.4116\n",
            "[25600/60000 ( 43%)]  Loss: 0.2895\n",
            "[38400/60000 ( 64%)]  Loss: 0.2048\n",
            "[51200/60000 ( 85%)]  Loss: 0.2552\n",
            "\n",
            "Average test loss: 0.1609  Accuracy: 9495/10000 (94.95%)\n",
            "\n",
            "Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.2338\n",
            "[12800/60000 ( 21%)]  Loss: 0.1619\n",
            "[25600/60000 ( 43%)]  Loss: 0.1619\n",
            "[38400/60000 ( 64%)]  Loss: 0.1482\n",
            "[51200/60000 ( 85%)]  Loss: 0.1054\n",
            "\n",
            "Average test loss: 0.1133  Accuracy: 9653/10000 (96.53%)\n",
            "\n",
            "Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.0721\n",
            "[12800/60000 ( 21%)]  Loss: 0.1308\n",
            "[25600/60000 ( 43%)]  Loss: 0.1191\n",
            "[38400/60000 ( 64%)]  Loss: 0.0908\n",
            "[51200/60000 ( 85%)]  Loss: 0.0423\n",
            "\n",
            "Average test loss: 0.1161  Accuracy: 9614/10000 (96.14%)\n",
            "\n",
            "Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.1752\n",
            "[12800/60000 ( 21%)]  Loss: 0.0135\n",
            "[25600/60000 ( 43%)]  Loss: 0.0874\n",
            "[38400/60000 ( 64%)]  Loss: 0.0105\n",
            "[51200/60000 ( 85%)]  Loss: 0.1475\n",
            "\n",
            "Average test loss: 0.0829  Accuracy: 9762/10000 (97.62%)\n",
            "\n",
            "Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.0380\n",
            "[12800/60000 ( 21%)]  Loss: 0.0937\n",
            "[25600/60000 ( 43%)]  Loss: 0.0750\n",
            "[38400/60000 ( 64%)]  Loss: 0.0376\n",
            "[51200/60000 ( 85%)]  Loss: 0.0101\n",
            "\n",
            "Average test loss: 0.0752  Accuracy: 9780/10000 (97.80%)\n",
            "\n",
            "Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.0424\n",
            "[12800/60000 ( 21%)]  Loss: 0.0744\n",
            "[25600/60000 ( 43%)]  Loss: 0.1508\n",
            "[38400/60000 ( 64%)]  Loss: 0.0475\n",
            "[51200/60000 ( 85%)]  Loss: 0.0970\n",
            "\n",
            "Average test loss: 0.0630  Accuracy: 9795/10000 (97.95%)\n",
            "\n",
            "Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.0144\n",
            "[12800/60000 ( 21%)]  Loss: 0.0280\n",
            "[25600/60000 ( 43%)]  Loss: 0.0790\n",
            "[38400/60000 ( 64%)]  Loss: 0.0214\n",
            "[51200/60000 ( 85%)]  Loss: 0.0716\n",
            "\n",
            "Average test loss: 0.0670  Accuracy: 9797/10000 (97.97%)\n",
            "\n",
            "Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.0060\n",
            "[12800/60000 ( 21%)]  Loss: 0.0349\n",
            "[25600/60000 ( 43%)]  Loss: 0.0237\n",
            "[38400/60000 ( 64%)]  Loss: 0.0129\n",
            "[51200/60000 ( 85%)]  Loss: 0.0018\n",
            "\n",
            "Average test loss: 0.0593  Accuracy: 9820/10000 (98.20%)\n",
            "\n",
            "Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0793\n",
            "[12800/60000 ( 21%)]  Loss: 0.0535\n",
            "[25600/60000 ( 43%)]  Loss: 0.0177\n",
            "[38400/60000 ( 64%)]  Loss: 0.0197\n",
            "[51200/60000 ( 85%)]  Loss: 0.0228\n",
            "\n",
            "Average test loss: 0.0658  Accuracy: 9803/10000 (98.03%)\n",
            "\n",
            "Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0203\n",
            "[12800/60000 ( 21%)]  Loss: 0.0127\n",
            "[25600/60000 ( 43%)]  Loss: 0.0211\n",
            "[38400/60000 ( 64%)]  Loss: 0.0722\n",
            "[51200/60000 ( 85%)]  Loss: 0.0491\n",
            "\n",
            "Average test loss: 0.0707  Accuracy: 9811/10000 (98.11%)\n",
            "\n",
            "Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.0072\n",
            "[12800/60000 ( 21%)]  Loss: 0.0003\n",
            "[25600/60000 ( 43%)]  Loss: 0.0239\n",
            "[38400/60000 ( 64%)]  Loss: 0.0178\n",
            "[51200/60000 ( 85%)]  Loss: 0.0055\n",
            "\n",
            "Average test loss: 0.0657  Accuracy: 9815/10000 (98.15%)\n",
            "\n",
            "Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0028\n",
            "[12800/60000 ( 21%)]  Loss: 0.0157\n",
            "[25600/60000 ( 43%)]  Loss: 0.0046\n",
            "[38400/60000 ( 64%)]  Loss: 0.0046\n",
            "[51200/60000 ( 85%)]  Loss: 0.0139\n",
            "\n",
            "Average test loss: 0.0605  Accuracy: 9837/10000 (98.37%)\n",
            "\n",
            "Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0083\n",
            "[12800/60000 ( 21%)]  Loss: 0.0019\n",
            "[25600/60000 ( 43%)]  Loss: 0.0021\n",
            "[38400/60000 ( 64%)]  Loss: 0.0212\n",
            "[51200/60000 ( 85%)]  Loss: 0.0094\n",
            "\n",
            "Average test loss: 0.0575  Accuracy: 9844/10000 (98.44%)\n",
            "\n",
            "Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0037\n",
            "[12800/60000 ( 21%)]  Loss: 0.0026\n",
            "[25600/60000 ( 43%)]  Loss: 0.0088\n",
            "[38400/60000 ( 64%)]  Loss: 0.0066\n",
            "[51200/60000 ( 85%)]  Loss: 0.0011\n",
            "\n",
            "Average test loss: 0.0759  Accuracy: 9808/10000 (98.08%)\n",
            "\n",
            "Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.1125\n",
            "[12800/60000 ( 21%)]  Loss: 0.0049\n",
            "[25600/60000 ( 43%)]  Loss: 0.0220\n",
            "[38400/60000 ( 64%)]  Loss: 0.0003\n",
            "[51200/60000 ( 85%)]  Loss: 0.0569\n",
            "\n",
            "Average test loss: 0.0545  Accuracy: 9856/10000 (98.56%)\n",
            "\n",
            "Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0185\n",
            "[12800/60000 ( 21%)]  Loss: 0.0064\n",
            "[25600/60000 ( 43%)]  Loss: 0.0015\n",
            "[38400/60000 ( 64%)]  Loss: 0.0014\n",
            "[51200/60000 ( 85%)]  Loss: 0.0005\n",
            "\n",
            "Average test loss: 0.0594  Accuracy: 9842/10000 (98.42%)\n",
            "\n",
            "Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0056\n",
            "[12800/60000 ( 21%)]  Loss: 0.0029\n",
            "[25600/60000 ( 43%)]  Loss: 0.0110\n",
            "[38400/60000 ( 64%)]  Loss: 0.0009\n",
            "[51200/60000 ( 85%)]  Loss: 0.0106\n",
            "\n",
            "Average test loss: 0.0620  Accuracy: 9854/10000 (98.54%)\n",
            "\n",
            "Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0502\n",
            "[12800/60000 ( 21%)]  Loss: 0.0001\n",
            "[25600/60000 ( 43%)]  Loss: 0.0026\n",
            "[38400/60000 ( 64%)]  Loss: 0.0102\n",
            "[51200/60000 ( 85%)]  Loss: 0.0099\n",
            "\n",
            "Average test loss: 0.0699  Accuracy: 9842/10000 (98.42%)\n",
            "\n",
            "Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0304\n",
            "[12800/60000 ( 21%)]  Loss: 0.0006\n",
            "[25600/60000 ( 43%)]  Loss: 0.0041\n",
            "[38400/60000 ( 64%)]  Loss: 0.0015\n",
            "[51200/60000 ( 85%)]  Loss: 0.0046\n",
            "\n",
            "Average test loss: 0.0701  Accuracy: 9848/10000 (98.48%)\n",
            "\n",
            "Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0009\n",
            "[12800/60000 ( 21%)]  Loss: 0.0008\n",
            "[25600/60000 ( 43%)]  Loss: 0.0006\n",
            "[38400/60000 ( 64%)]  Loss: 0.0038\n",
            "[51200/60000 ( 85%)]  Loss: 0.0001\n",
            "\n",
            "Average test loss: 0.0656  Accuracy: 9860/10000 (98.60%)\n",
            "\n",
            "Execution time: 455.04 seconds\n",
            "-----------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------\n",
            "Running experiment with dim=32, depth=4, heads=8, mlp_dim=512\n",
            "Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.3227\n",
            "[12800/60000 ( 21%)]  Loss: 1.2977\n",
            "[25600/60000 ( 43%)]  Loss: 0.7003\n",
            "[38400/60000 ( 64%)]  Loss: 0.6300\n",
            "[51200/60000 ( 85%)]  Loss: 0.2936\n",
            "\n",
            "Average test loss: 0.3253  Accuracy: 8955/10000 (89.55%)\n",
            "\n",
            "Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.2909\n",
            "[12800/60000 ( 21%)]  Loss: 0.3544\n",
            "[25600/60000 ( 43%)]  Loss: 0.3215\n",
            "[38400/60000 ( 64%)]  Loss: 0.2317\n",
            "[51200/60000 ( 85%)]  Loss: 0.1886\n",
            "\n",
            "Average test loss: 0.2280  Accuracy: 9268/10000 (92.68%)\n",
            "\n",
            "Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.2379\n",
            "[12800/60000 ( 21%)]  Loss: 0.1930\n",
            "[25600/60000 ( 43%)]  Loss: 0.1629\n",
            "[38400/60000 ( 64%)]  Loss: 0.1356\n",
            "[51200/60000 ( 85%)]  Loss: 0.1750\n",
            "\n",
            "Average test loss: 0.1365  Accuracy: 9580/10000 (95.80%)\n",
            "\n",
            "Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.2087\n",
            "[12800/60000 ( 21%)]  Loss: 0.0666\n",
            "[25600/60000 ( 43%)]  Loss: 0.0803\n",
            "[38400/60000 ( 64%)]  Loss: 0.0712\n",
            "[51200/60000 ( 85%)]  Loss: 0.1804\n",
            "\n",
            "Average test loss: 0.1215  Accuracy: 9625/10000 (96.25%)\n",
            "\n",
            "Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.1390\n",
            "[12800/60000 ( 21%)]  Loss: 0.1213\n",
            "[25600/60000 ( 43%)]  Loss: 0.1507\n",
            "[38400/60000 ( 64%)]  Loss: 0.1021\n",
            "[51200/60000 ( 85%)]  Loss: 0.0515\n",
            "\n",
            "Average test loss: 0.1059  Accuracy: 9671/10000 (96.71%)\n",
            "\n",
            "Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.0953\n",
            "[12800/60000 ( 21%)]  Loss: 0.1239\n",
            "[25600/60000 ( 43%)]  Loss: 0.0730\n",
            "[38400/60000 ( 64%)]  Loss: 0.0379\n",
            "[51200/60000 ( 85%)]  Loss: 0.0680\n",
            "\n",
            "Average test loss: 0.0868  Accuracy: 9713/10000 (97.13%)\n",
            "\n",
            "Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.0375\n",
            "[12800/60000 ( 21%)]  Loss: 0.0286\n",
            "[25600/60000 ( 43%)]  Loss: 0.0264\n",
            "[38400/60000 ( 64%)]  Loss: 0.0484\n",
            "[51200/60000 ( 85%)]  Loss: 0.1330\n",
            "\n",
            "Average test loss: 0.0774  Accuracy: 9759/10000 (97.59%)\n",
            "\n",
            "Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.0760\n",
            "[12800/60000 ( 21%)]  Loss: 0.0513\n",
            "[25600/60000 ( 43%)]  Loss: 0.0236\n",
            "[38400/60000 ( 64%)]  Loss: 0.0754\n",
            "[51200/60000 ( 85%)]  Loss: 0.0396\n",
            "\n",
            "Average test loss: 0.0867  Accuracy: 9722/10000 (97.22%)\n",
            "\n",
            "Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0812\n",
            "[12800/60000 ( 21%)]  Loss: 0.0452\n",
            "[25600/60000 ( 43%)]  Loss: 0.0072\n",
            "[38400/60000 ( 64%)]  Loss: 0.1062\n",
            "[51200/60000 ( 85%)]  Loss: 0.0440\n",
            "\n",
            "Average test loss: 0.0737  Accuracy: 9748/10000 (97.48%)\n",
            "\n",
            "Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0217\n",
            "[12800/60000 ( 21%)]  Loss: 0.0607\n",
            "[25600/60000 ( 43%)]  Loss: 0.0163\n",
            "[38400/60000 ( 64%)]  Loss: 0.0507\n",
            "[51200/60000 ( 85%)]  Loss: 0.0767\n",
            "\n",
            "Average test loss: 0.0652  Accuracy: 9795/10000 (97.95%)\n",
            "\n",
            "Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.0101\n",
            "[12800/60000 ( 21%)]  Loss: 0.0408\n",
            "[25600/60000 ( 43%)]  Loss: 0.0747\n",
            "[38400/60000 ( 64%)]  Loss: 0.1194\n",
            "[51200/60000 ( 85%)]  Loss: 0.0144\n",
            "\n",
            "Average test loss: 0.0660  Accuracy: 9796/10000 (97.96%)\n",
            "\n",
            "Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0163\n",
            "[12800/60000 ( 21%)]  Loss: 0.0522\n",
            "[25600/60000 ( 43%)]  Loss: 0.0187\n",
            "[38400/60000 ( 64%)]  Loss: 0.0385\n",
            "[51200/60000 ( 85%)]  Loss: 0.0257\n",
            "\n",
            "Average test loss: 0.0730  Accuracy: 9777/10000 (97.77%)\n",
            "\n",
            "Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0160\n",
            "[12800/60000 ( 21%)]  Loss: 0.0557\n",
            "[25600/60000 ( 43%)]  Loss: 0.0598\n",
            "[38400/60000 ( 64%)]  Loss: 0.0209\n",
            "[51200/60000 ( 85%)]  Loss: 0.0177\n",
            "\n",
            "Average test loss: 0.0782  Accuracy: 9776/10000 (97.76%)\n",
            "\n",
            "Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0193\n",
            "[12800/60000 ( 21%)]  Loss: 0.0477\n",
            "[25600/60000 ( 43%)]  Loss: 0.0303\n",
            "[38400/60000 ( 64%)]  Loss: 0.0462\n",
            "[51200/60000 ( 85%)]  Loss: 0.0130\n",
            "\n",
            "Average test loss: 0.0772  Accuracy: 9758/10000 (97.58%)\n",
            "\n",
            "Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0081\n",
            "[12800/60000 ( 21%)]  Loss: 0.0297\n",
            "[25600/60000 ( 43%)]  Loss: 0.0742\n",
            "[38400/60000 ( 64%)]  Loss: 0.0039\n",
            "[51200/60000 ( 85%)]  Loss: 0.0435\n",
            "\n",
            "Average test loss: 0.0647  Accuracy: 9802/10000 (98.02%)\n",
            "\n",
            "Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0424\n",
            "[12800/60000 ( 21%)]  Loss: 0.0031\n",
            "[25600/60000 ( 43%)]  Loss: 0.0177\n",
            "[38400/60000 ( 64%)]  Loss: 0.0250\n",
            "[51200/60000 ( 85%)]  Loss: 0.0525\n",
            "\n",
            "Average test loss: 0.0664  Accuracy: 9802/10000 (98.02%)\n",
            "\n",
            "Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0035\n",
            "[12800/60000 ( 21%)]  Loss: 0.0130\n",
            "[25600/60000 ( 43%)]  Loss: 0.0699\n",
            "[38400/60000 ( 64%)]  Loss: 0.0107\n",
            "[51200/60000 ( 85%)]  Loss: 0.0068\n",
            "\n",
            "Average test loss: 0.0933  Accuracy: 9757/10000 (97.57%)\n",
            "\n",
            "Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0300\n",
            "[12800/60000 ( 21%)]  Loss: 0.0196\n",
            "[25600/60000 ( 43%)]  Loss: 0.0389\n",
            "[38400/60000 ( 64%)]  Loss: 0.0327\n",
            "[51200/60000 ( 85%)]  Loss: 0.0073\n",
            "\n",
            "Average test loss: 0.0767  Accuracy: 9793/10000 (97.93%)\n",
            "\n",
            "Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0056\n",
            "[12800/60000 ( 21%)]  Loss: 0.0030\n",
            "[25600/60000 ( 43%)]  Loss: 0.0238\n",
            "[38400/60000 ( 64%)]  Loss: 0.0121\n",
            "[51200/60000 ( 85%)]  Loss: 0.0229\n",
            "\n",
            "Average test loss: 0.0764  Accuracy: 9798/10000 (97.98%)\n",
            "\n",
            "Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0422\n",
            "[12800/60000 ( 21%)]  Loss: 0.0104\n",
            "[25600/60000 ( 43%)]  Loss: 0.0143\n",
            "[38400/60000 ( 64%)]  Loss: 0.0102\n",
            "[51200/60000 ( 85%)]  Loss: 0.0047\n",
            "\n",
            "Average test loss: 0.0810  Accuracy: 9808/10000 (98.08%)\n",
            "\n",
            "Execution time: 301.94 seconds\n",
            "-----------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------\n",
            "Running experiment with dim=128, depth=10, heads=8, mlp_dim=128\n",
            "Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.3852\n",
            "[12800/60000 ( 21%)]  Loss: 0.7725\n",
            "[25600/60000 ( 43%)]  Loss: 0.2847\n",
            "[38400/60000 ( 64%)]  Loss: 0.2833\n",
            "[51200/60000 ( 85%)]  Loss: 0.1424\n",
            "\n",
            "Average test loss: 0.1565  Accuracy: 9513/10000 (95.13%)\n",
            "\n",
            "Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.1050\n",
            "[12800/60000 ( 21%)]  Loss: 0.1362\n",
            "[25600/60000 ( 43%)]  Loss: 0.0848\n",
            "[38400/60000 ( 64%)]  Loss: 0.0372\n",
            "[51200/60000 ( 85%)]  Loss: 0.0571\n",
            "\n",
            "Average test loss: 0.0982  Accuracy: 9675/10000 (96.75%)\n",
            "\n",
            "Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.0653\n",
            "[12800/60000 ( 21%)]  Loss: 0.0474\n",
            "[25600/60000 ( 43%)]  Loss: 0.0981\n",
            "[38400/60000 ( 64%)]  Loss: 0.0475\n",
            "[51200/60000 ( 85%)]  Loss: 0.0231\n",
            "\n",
            "Average test loss: 0.0881  Accuracy: 9727/10000 (97.27%)\n",
            "\n",
            "Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.0570\n",
            "[12800/60000 ( 21%)]  Loss: 0.0460\n",
            "[25600/60000 ( 43%)]  Loss: 0.0643\n",
            "[38400/60000 ( 64%)]  Loss: 0.0754\n",
            "[51200/60000 ( 85%)]  Loss: 0.2015\n",
            "\n",
            "Average test loss: 0.0702  Accuracy: 9761/10000 (97.61%)\n",
            "\n",
            "Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.0564\n",
            "[12800/60000 ( 21%)]  Loss: 0.0796\n",
            "[25600/60000 ( 43%)]  Loss: 0.0489\n",
            "[38400/60000 ( 64%)]  Loss: 0.0291\n",
            "[51200/60000 ( 85%)]  Loss: 0.1317\n",
            "\n",
            "Average test loss: 0.0762  Accuracy: 9758/10000 (97.58%)\n",
            "\n",
            "Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.0267\n",
            "[12800/60000 ( 21%)]  Loss: 0.0069\n",
            "[25600/60000 ( 43%)]  Loss: 0.0352\n",
            "[38400/60000 ( 64%)]  Loss: 0.0128\n",
            "[51200/60000 ( 85%)]  Loss: 0.0094\n",
            "\n",
            "Average test loss: 0.0703  Accuracy: 9790/10000 (97.90%)\n",
            "\n",
            "Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.0100\n",
            "[12800/60000 ( 21%)]  Loss: 0.0523\n",
            "[25600/60000 ( 43%)]  Loss: 0.0097\n",
            "[38400/60000 ( 64%)]  Loss: 0.0285\n",
            "[51200/60000 ( 85%)]  Loss: 0.1404\n",
            "\n",
            "Average test loss: 0.0665  Accuracy: 9805/10000 (98.05%)\n",
            "\n",
            "Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.0072\n",
            "[12800/60000 ( 21%)]  Loss: 0.0320\n",
            "[25600/60000 ( 43%)]  Loss: 0.0264\n",
            "[38400/60000 ( 64%)]  Loss: 0.0483\n",
            "[51200/60000 ( 85%)]  Loss: 0.0581\n",
            "\n",
            "Average test loss: 0.0609  Accuracy: 9828/10000 (98.28%)\n",
            "\n",
            "Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0413\n",
            "[12800/60000 ( 21%)]  Loss: 0.0067\n",
            "[25600/60000 ( 43%)]  Loss: 0.0203\n",
            "[38400/60000 ( 64%)]  Loss: 0.0088\n",
            "[51200/60000 ( 85%)]  Loss: 0.0306\n",
            "\n",
            "Average test loss: 0.0624  Accuracy: 9826/10000 (98.26%)\n",
            "\n",
            "Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0299\n",
            "[12800/60000 ( 21%)]  Loss: 0.0228\n",
            "[25600/60000 ( 43%)]  Loss: 0.0272\n",
            "[38400/60000 ( 64%)]  Loss: 0.0201\n",
            "[51200/60000 ( 85%)]  Loss: 0.0319\n",
            "\n",
            "Average test loss: 0.0622  Accuracy: 9834/10000 (98.34%)\n",
            "\n",
            "Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.0070\n",
            "[12800/60000 ( 21%)]  Loss: 0.0380\n",
            "[25600/60000 ( 43%)]  Loss: 0.0412\n",
            "[38400/60000 ( 64%)]  Loss: 0.0361\n",
            "[51200/60000 ( 85%)]  Loss: 0.0029\n",
            "\n",
            "Average test loss: 0.0615  Accuracy: 9840/10000 (98.40%)\n",
            "\n",
            "Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0075\n",
            "[12800/60000 ( 21%)]  Loss: 0.0015\n",
            "[25600/60000 ( 43%)]  Loss: 0.1320\n",
            "[38400/60000 ( 64%)]  Loss: 0.0024\n",
            "[51200/60000 ( 85%)]  Loss: 0.0017\n",
            "\n",
            "Average test loss: 0.0620  Accuracy: 9838/10000 (98.38%)\n",
            "\n",
            "Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0146\n",
            "[12800/60000 ( 21%)]  Loss: 0.0024\n",
            "[25600/60000 ( 43%)]  Loss: 0.0211\n",
            "[38400/60000 ( 64%)]  Loss: 0.0018\n",
            "[51200/60000 ( 85%)]  Loss: 0.0136\n",
            "\n",
            "Average test loss: 0.0714  Accuracy: 9829/10000 (98.29%)\n",
            "\n",
            "Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0009\n",
            "[12800/60000 ( 21%)]  Loss: 0.0095\n",
            "[25600/60000 ( 43%)]  Loss: 0.0573\n",
            "[38400/60000 ( 64%)]  Loss: 0.0037\n",
            "[51200/60000 ( 85%)]  Loss: 0.0135\n",
            "\n",
            "Average test loss: 0.0700  Accuracy: 9813/10000 (98.13%)\n",
            "\n",
            "Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0044\n",
            "[12800/60000 ( 21%)]  Loss: 0.0096\n",
            "[25600/60000 ( 43%)]  Loss: 0.0071\n",
            "[38400/60000 ( 64%)]  Loss: 0.0027\n",
            "[51200/60000 ( 85%)]  Loss: 0.0093\n",
            "\n",
            "Average test loss: 0.0702  Accuracy: 9840/10000 (98.40%)\n",
            "\n",
            "Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0000\n",
            "[12800/60000 ( 21%)]  Loss: 0.0044\n",
            "[25600/60000 ( 43%)]  Loss: 0.0024\n",
            "[38400/60000 ( 64%)]  Loss: 0.0055\n",
            "[51200/60000 ( 85%)]  Loss: 0.0003\n",
            "\n",
            "Average test loss: 0.0757  Accuracy: 9834/10000 (98.34%)\n",
            "\n",
            "Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0030\n",
            "[12800/60000 ( 21%)]  Loss: 0.0008\n",
            "[25600/60000 ( 43%)]  Loss: 0.0072\n",
            "[38400/60000 ( 64%)]  Loss: 0.0012\n",
            "[51200/60000 ( 85%)]  Loss: 0.0017\n",
            "\n",
            "Average test loss: 0.0723  Accuracy: 9844/10000 (98.44%)\n",
            "\n",
            "Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0297\n",
            "[12800/60000 ( 21%)]  Loss: 0.0037\n",
            "[25600/60000 ( 43%)]  Loss: 0.0005\n",
            "[38400/60000 ( 64%)]  Loss: 0.0002\n",
            "[51200/60000 ( 85%)]  Loss: 0.0004\n",
            "\n",
            "Average test loss: 0.0586  Accuracy: 9863/10000 (98.63%)\n",
            "\n",
            "Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0018\n",
            "[12800/60000 ( 21%)]  Loss: 0.0016\n",
            "[25600/60000 ( 43%)]  Loss: 0.0002\n",
            "[38400/60000 ( 64%)]  Loss: 0.0119\n",
            "[51200/60000 ( 85%)]  Loss: 0.0011\n",
            "\n",
            "Average test loss: 0.0727  Accuracy: 9863/10000 (98.63%)\n",
            "\n",
            "Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0119\n",
            "[12800/60000 ( 21%)]  Loss: 0.0003\n",
            "[25600/60000 ( 43%)]  Loss: 0.0041\n",
            "[38400/60000 ( 64%)]  Loss: 0.0018\n",
            "[51200/60000 ( 85%)]  Loss: 0.0021\n",
            "\n",
            "Average test loss: 0.0787  Accuracy: 9833/10000 (98.33%)\n",
            "\n",
            "Execution time: 758.09 seconds\n",
            "-----------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------\n",
            "Running experiment with dim=256, depth=10, heads=8, mlp_dim=256\n",
            "Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.3516\n",
            "[12800/60000 ( 21%)]  Loss: 0.4177\n",
            "[25600/60000 ( 43%)]  Loss: 0.1134\n",
            "[38400/60000 ( 64%)]  Loss: 0.2815\n",
            "[51200/60000 ( 85%)]  Loss: 0.2022\n",
            "\n",
            "Average test loss: 0.1496  Accuracy: 9541/10000 (95.41%)\n",
            "\n",
            "Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.1880\n",
            "[12800/60000 ( 21%)]  Loss: 0.0768\n",
            "[25600/60000 ( 43%)]  Loss: 0.0824\n",
            "[38400/60000 ( 64%)]  Loss: 0.1465\n",
            "[51200/60000 ( 85%)]  Loss: 0.0850\n",
            "\n",
            "Average test loss: 0.1346  Accuracy: 9599/10000 (95.99%)\n",
            "\n",
            "Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.1088\n",
            "[12800/60000 ( 21%)]  Loss: 0.1605\n",
            "[25600/60000 ( 43%)]  Loss: 0.1593\n",
            "[38400/60000 ( 64%)]  Loss: 0.0397\n",
            "[51200/60000 ( 85%)]  Loss: 0.0922\n",
            "\n",
            "Average test loss: 0.0990  Accuracy: 9696/10000 (96.96%)\n",
            "\n",
            "Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.1388\n",
            "[12800/60000 ( 21%)]  Loss: 0.0722\n",
            "[25600/60000 ( 43%)]  Loss: 0.0696\n",
            "[38400/60000 ( 64%)]  Loss: 0.0702\n",
            "[51200/60000 ( 85%)]  Loss: 0.0960\n",
            "\n",
            "Average test loss: 0.0867  Accuracy: 9722/10000 (97.22%)\n",
            "\n",
            "Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.1460\n",
            "[12800/60000 ( 21%)]  Loss: 0.0953\n",
            "[25600/60000 ( 43%)]  Loss: 0.0334\n",
            "[38400/60000 ( 64%)]  Loss: 0.0365\n",
            "[51200/60000 ( 85%)]  Loss: 0.1333\n",
            "\n",
            "Average test loss: 0.0893  Accuracy: 9704/10000 (97.04%)\n",
            "\n",
            "Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.0628\n",
            "[12800/60000 ( 21%)]  Loss: 0.1441\n",
            "[25600/60000 ( 43%)]  Loss: 0.1209\n",
            "[38400/60000 ( 64%)]  Loss: 0.0511\n",
            "[51200/60000 ( 85%)]  Loss: 0.1110\n",
            "\n",
            "Average test loss: 0.0720  Accuracy: 9787/10000 (97.87%)\n",
            "\n",
            "Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.0167\n",
            "[12800/60000 ( 21%)]  Loss: 0.0266\n",
            "[25600/60000 ( 43%)]  Loss: 0.0560\n",
            "[38400/60000 ( 64%)]  Loss: 0.0778\n",
            "[51200/60000 ( 85%)]  Loss: 0.1410\n",
            "\n",
            "Average test loss: 0.0835  Accuracy: 9751/10000 (97.51%)\n",
            "\n",
            "Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.0318\n",
            "[12800/60000 ( 21%)]  Loss: 0.0759\n",
            "[25600/60000 ( 43%)]  Loss: 0.1046\n",
            "[38400/60000 ( 64%)]  Loss: 0.0228\n",
            "[51200/60000 ( 85%)]  Loss: 0.0873\n",
            "\n",
            "Average test loss: 0.0743  Accuracy: 9798/10000 (97.98%)\n",
            "\n",
            "Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0240\n",
            "[12800/60000 ( 21%)]  Loss: 0.0234\n",
            "[25600/60000 ( 43%)]  Loss: 0.0138\n",
            "[38400/60000 ( 64%)]  Loss: 0.0214\n",
            "[51200/60000 ( 85%)]  Loss: 0.0211\n",
            "\n",
            "Average test loss: 0.0764  Accuracy: 9792/10000 (97.92%)\n",
            "\n",
            "Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0058\n",
            "[12800/60000 ( 21%)]  Loss: 0.0048\n",
            "[25600/60000 ( 43%)]  Loss: 0.0711\n",
            "[38400/60000 ( 64%)]  Loss: 0.0772\n",
            "[51200/60000 ( 85%)]  Loss: 0.0464\n",
            "\n",
            "Average test loss: 0.0635  Accuracy: 9811/10000 (98.11%)\n",
            "\n",
            "Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.0184\n",
            "[12800/60000 ( 21%)]  Loss: 0.0734\n",
            "[25600/60000 ( 43%)]  Loss: 0.0146\n",
            "[38400/60000 ( 64%)]  Loss: 0.1045\n",
            "[51200/60000 ( 85%)]  Loss: 0.0800\n",
            "\n",
            "Average test loss: 0.0680  Accuracy: 9809/10000 (98.09%)\n",
            "\n",
            "Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0095\n",
            "[12800/60000 ( 21%)]  Loss: 0.0112\n",
            "[25600/60000 ( 43%)]  Loss: 0.0065\n",
            "[38400/60000 ( 64%)]  Loss: 0.0029\n",
            "[51200/60000 ( 85%)]  Loss: 0.0032\n",
            "\n",
            "Average test loss: 0.0684  Accuracy: 9814/10000 (98.14%)\n",
            "\n",
            "Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0072\n",
            "[12800/60000 ( 21%)]  Loss: 0.0042\n",
            "[25600/60000 ( 43%)]  Loss: 0.1092\n",
            "[38400/60000 ( 64%)]  Loss: 0.0889\n",
            "[51200/60000 ( 85%)]  Loss: 0.0215\n",
            "\n",
            "Average test loss: 0.0703  Accuracy: 9832/10000 (98.32%)\n",
            "\n",
            "Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0072\n",
            "[12800/60000 ( 21%)]  Loss: 0.0123\n",
            "[25600/60000 ( 43%)]  Loss: 0.0287\n",
            "[38400/60000 ( 64%)]  Loss: 0.0576\n",
            "[51200/60000 ( 85%)]  Loss: 0.0040\n",
            "\n",
            "Average test loss: 0.0761  Accuracy: 9809/10000 (98.09%)\n",
            "\n",
            "Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0615\n",
            "[12800/60000 ( 21%)]  Loss: 0.0087\n",
            "[25600/60000 ( 43%)]  Loss: 0.0136\n",
            "[38400/60000 ( 64%)]  Loss: 0.0045\n",
            "[51200/60000 ( 85%)]  Loss: 0.0211\n",
            "\n",
            "Average test loss: 0.0796  Accuracy: 9820/10000 (98.20%)\n",
            "\n",
            "Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0072\n",
            "[12800/60000 ( 21%)]  Loss: 0.0056\n",
            "[25600/60000 ( 43%)]  Loss: 0.0129\n",
            "[38400/60000 ( 64%)]  Loss: 0.0105\n",
            "[51200/60000 ( 85%)]  Loss: 0.0161\n",
            "\n",
            "Average test loss: 0.0751  Accuracy: 9822/10000 (98.22%)\n",
            "\n",
            "Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0693\n",
            "[12800/60000 ( 21%)]  Loss: 0.0118\n",
            "[25600/60000 ( 43%)]  Loss: 0.0013\n",
            "[38400/60000 ( 64%)]  Loss: 0.0070\n",
            "[51200/60000 ( 85%)]  Loss: 0.0293\n",
            "\n",
            "Average test loss: 0.0691  Accuracy: 9834/10000 (98.34%)\n",
            "\n",
            "Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0016\n",
            "[12800/60000 ( 21%)]  Loss: 0.0013\n",
            "[25600/60000 ( 43%)]  Loss: 0.0001\n",
            "[38400/60000 ( 64%)]  Loss: 0.0001\n",
            "[51200/60000 ( 85%)]  Loss: 0.0090\n",
            "\n",
            "Average test loss: 0.0863  Accuracy: 9826/10000 (98.26%)\n",
            "\n",
            "Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0209\n",
            "[12800/60000 ( 21%)]  Loss: 0.0250\n",
            "[25600/60000 ( 43%)]  Loss: 0.0006\n",
            "[38400/60000 ( 64%)]  Loss: 0.0007\n",
            "[51200/60000 ( 85%)]  Loss: 0.0393\n",
            "\n",
            "Average test loss: 0.0668  Accuracy: 9846/10000 (98.46%)\n",
            "\n",
            "Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0180\n",
            "[12800/60000 ( 21%)]  Loss: 0.0059\n",
            "[25600/60000 ( 43%)]  Loss: 0.0002\n",
            "[38400/60000 ( 64%)]  Loss: 0.0035\n",
            "[51200/60000 ( 85%)]  Loss: 0.0776\n",
            "\n",
            "Average test loss: 0.0853  Accuracy: 9815/10000 (98.15%)\n",
            "\n",
            "Execution time: 1432.34 seconds\n",
            "-----------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------\n",
            "Running experiment with dim=256, depth=8, heads=8, mlp_dim=512\n",
            "Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.3767\n",
            "[12800/60000 ( 21%)]  Loss: 0.4915\n",
            "[25600/60000 ( 43%)]  Loss: 0.3357\n",
            "[38400/60000 ( 64%)]  Loss: 0.1761\n",
            "[51200/60000 ( 85%)]  Loss: 0.1730\n",
            "\n",
            "Average test loss: 0.1424  Accuracy: 9560/10000 (95.60%)\n",
            "\n",
            "Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.1749\n",
            "[12800/60000 ( 21%)]  Loss: 0.1694\n",
            "[25600/60000 ( 43%)]  Loss: 0.2256\n",
            "[38400/60000 ( 64%)]  Loss: 0.0773\n",
            "[51200/60000 ( 85%)]  Loss: 0.1490\n",
            "\n",
            "Average test loss: 0.0990  Accuracy: 9722/10000 (97.22%)\n",
            "\n",
            "Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.0393\n",
            "[12800/60000 ( 21%)]  Loss: 0.1109\n",
            "[25600/60000 ( 43%)]  Loss: 0.1029\n",
            "[38400/60000 ( 64%)]  Loss: 0.2019\n",
            "[51200/60000 ( 85%)]  Loss: 0.1785\n",
            "\n",
            "Average test loss: 0.0997  Accuracy: 9713/10000 (97.13%)\n",
            "\n",
            "Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.0508\n",
            "[12800/60000 ( 21%)]  Loss: 0.1431\n",
            "[25600/60000 ( 43%)]  Loss: 0.0910\n",
            "[38400/60000 ( 64%)]  Loss: 0.1410\n",
            "[51200/60000 ( 85%)]  Loss: 0.0608\n",
            "\n",
            "Average test loss: 0.0834  Accuracy: 9737/10000 (97.37%)\n",
            "\n",
            "Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.0839\n",
            "[12800/60000 ( 21%)]  Loss: 0.0893\n",
            "[25600/60000 ( 43%)]  Loss: 0.0948\n",
            "[38400/60000 ( 64%)]  Loss: 0.0688\n",
            "[51200/60000 ( 85%)]  Loss: 0.0224\n",
            "\n",
            "Average test loss: 0.0867  Accuracy: 9749/10000 (97.49%)\n",
            "\n",
            "Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.0766\n",
            "[12800/60000 ( 21%)]  Loss: 0.0522\n",
            "[25600/60000 ( 43%)]  Loss: 0.0682\n",
            "[38400/60000 ( 64%)]  Loss: 0.0726\n",
            "[51200/60000 ( 85%)]  Loss: 0.0350\n",
            "\n",
            "Average test loss: 0.0853  Accuracy: 9746/10000 (97.46%)\n",
            "\n",
            "Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.0323\n",
            "[12800/60000 ( 21%)]  Loss: 0.0322\n",
            "[25600/60000 ( 43%)]  Loss: 0.1603\n",
            "[38400/60000 ( 64%)]  Loss: 0.1090\n",
            "[51200/60000 ( 85%)]  Loss: 0.0183\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[40], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch:\u001b[39m\u001b[38;5;124m'\u001b[39m, epoch,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLR:\u001b[39m\u001b[38;5;124m'\u001b[39m, scheduler\u001b[38;5;241m.\u001b[39mget_last_lr())\n\u001b[0;32m     40\u001b[0m     train_epoch(model, optimizer, train_loader, train_loss_history)\n\u001b[1;32m---> 41\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loss_history\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     43\u001b[0m store[(dim, depth, heads, mlp_dim)] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: train_loss_history,\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: test_loss_history,\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: accuracy,\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexecution_time\u001b[39m\u001b[38;5;124m'\u001b[39m: time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m     48\u001b[0m }\n",
            "Cell \u001b[1;32mIn[27], line 14\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, data_loader, loss_history)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data, target \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;66;03m# data = data.cuda()\u001b[39;00m\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;66;03m# target = target.cuda()\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m         output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     15\u001b[0m         loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(output, target, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m         _, pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[1;32mc:\\Git\\kul-anndl-ss24\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Git\\kul-anndl-ss24\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[25], line 111\u001b[0m, in \u001b[0;36mViT.forward\u001b[1;34m(self, img, mask)\u001b[0m\n\u001b[0;32m    109\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((cls_tokens, x), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    110\u001b[0m x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding\n\u001b[1;32m--> 111\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_cls_token(x[:, \u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_head(x)\n",
            "File \u001b[1;32mc:\\Git\\kul-anndl-ss24\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Git\\kul-anndl-ss24\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[25], line 77\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attn, ff \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m     76\u001b[0m     x \u001b[38;5;241m=\u001b[39m attn(x, mask\u001b[38;5;241m=\u001b[39mmask)\n\u001b[1;32m---> 77\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[1;32mc:\\Git\\kul-anndl-ss24\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Git\\kul-anndl-ss24\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[25], line 11\u001b[0m, in \u001b[0;36mResidual.forward\u001b[1;34m(self, x, **kwargs)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m x\n",
            "File \u001b[1;32mc:\\Git\\kul-anndl-ss24\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Git\\kul-anndl-ss24\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[25], line 20\u001b[0m, in \u001b[0;36mPreNorm.forward\u001b[1;34m(self, x, **kwargs)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Git\\kul-anndl-ss24\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Git\\kul-anndl-ss24\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[25], line 32\u001b[0m, in \u001b[0;36mFeedForward.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Git\\kul-anndl-ss24\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Git\\kul-anndl-ss24\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Git\\kul-anndl-ss24\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Git\\kul-anndl-ss24\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Git\\kul-anndl-ss24\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Git\\kul-anndl-ss24\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import time\n",
        "from itertools import product\n",
        "\n",
        "range_dim = [32, 64, 128]\n",
        "range_depth = [4, 6, 8]\n",
        "range_heads = [8]\n",
        "range_mlp_dim = [64, 128, 256]\n",
        "\n",
        "add_range_dim = [256]\n",
        "add_range_depth = [10]\n",
        "add_range_heads = []\n",
        "add_range_mlp_dim = [512]\n",
        "\n",
        "store = Store('results.pkl')\n",
        "N_EPOCHS = 20\n",
        "\n",
        "# Shuffle the order of the experiments\n",
        "all_ranges = list(product(range_dim, range_depth, range_heads, range_mlp_dim))\n",
        "new_ranges = product(range_dim + add_range_dim, range_depth + add_range_depth,\n",
        "                      range_heads + add_range_heads, range_mlp_dim + add_range_mlp_dim)\n",
        "new_ranges = list(new_ranges)\n",
        "np.random.shuffle(new_ranges)\n",
        "\n",
        "for dim, depth, heads, mlp_dim in new_ranges:\n",
        "    if (dim, depth, heads, mlp_dim) in store:\n",
        "        continue\n",
        "    if (dim, depth, heads, mlp_dim) in all_ranges:\n",
        "        continue\n",
        "    print('-----------------------------------------------------------------------------------')\n",
        "    print(f'Running experiment with dim={dim}, depth={depth}, heads={heads}, mlp_dim={mlp_dim}')\n",
        "    model = ViT(image_size=28, patch_size=7, num_classes=10, channels=1,\n",
        "                dim=dim, depth=depth, heads=heads, mlp_dim=mlp_dim)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    train_loss_history, test_loss_history = [], []\n",
        "    train_loader, test_loader = get_mnist_loader(batch_size=128, shuffle=True)\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "    start_time = time.time()\n",
        "    for epoch in range(1, N_EPOCHS + 1):\n",
        "        print('Epoch:', epoch,'LR:', scheduler.get_last_lr())\n",
        "        train_epoch(model, optimizer, train_loader, train_loss_history)\n",
        "        accuracy = evaluate(model, test_loader, test_loss_history)\n",
        "        scheduler.step()\n",
        "    store[(dim, depth, heads, mlp_dim)] = {\n",
        "        'train_loss': train_loss_history,\n",
        "        'test_loss': test_loss_history,\n",
        "        'accuracy': float(accuracy),\n",
        "        'execution_time': time.time() - start_time\n",
        "    }\n",
        "    print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')\n",
        "    print('-----------------------------------------------------------------------------------')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\begin{table}[htbp]\n",
            "\t\\begin{center}\n",
            "\t\t\\begin{tabular}{|c|c|c|c|c|c|}\n",
            "\t\t\t\\hline\n",
            "\t\t\t\\textbf{Rank} & \\textbf{\\texttt{dim}} & \\textbf{\\texttt{depth}} & \\textbf{\\texttt{mlp\\_dim}} & \\textbf{\\texttt{accuracy}} & \\textbf{\\texttt{Exec Time}} \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t1 & 128 & 8 & 128 & 0.9871 & 351.1s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t2 & 128 & 6 & 256 & 0.9863 & 309.6s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t3 & 128 & 4 & 512 & 0.9860 & 455.0s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t4 & 256 & 4 & 512 & 0.9859 & 730.3s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t5 & 128 & 8 & 64 & 0.9857 & 351.4s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t6 & 64 & 8 & 128 & 0.9856 & 356.4s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t7 & 64 & 4 & 128 & 0.9851 & 264.4s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t8 & 64 & 8 & 256 & 0.9849 & 355.6s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t9 & 64 & 6 & 128 & 0.9847 & 310.6s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t10 & 32 & 8 & 128 & 0.9845 & 350.2s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t11 & 128 & 6 & 128 & 0.9842 & 500.1s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t12 & 128 & 8 & 256 & 0.9841 & 352.6s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t13 & 256 & 8 & 64 & 0.9839 & 999.3s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t14 & 128 & 10 & 64 & 0.9838 & 710.6s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t15 & 128 & 6 & 64 & 0.9837 & 311.1s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t16 & 128 & 4 & 256 & 0.9837 & 270.3s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t17 & 32 & 10 & 256 & 0.9835 & 605.7s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t18 & 64 & 8 & 64 & 0.9835 & 356.8s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t19 & 64 & 4 & 64 & 0.9834 & 281.3s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t20 & 128 & 10 & 128 & 0.9833 & 758.1s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t21 & 32 & 8 & 256 & 0.9833 & 353.7s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t22 & 128 & 4 & 128 & 0.9833 & 269.3s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t23 & 64 & 6 & 256 & 0.9831 & 461.7s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t24 & 256 & 10 & 128 & 0.9831 & 1302.0s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t25 & 128 & 4 & 64 & 0.9829 & 268.8s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t26 & 32 & 8 & 64 & 0.9829 & 351.6s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t27 & 256 & 8 & 128 & 0.9827 & 1067.3s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t28 & 64 & 6 & 64 & 0.9827 & 309.7s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t29 & 32 & 8 & 512 & 0.9820 & 529.6s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t30 & 32 & 6 & 128 & 0.9819 & 311.4s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t31 & 32 & 6 & 64 & 0.9816 & 313.3s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t32 & 64 & 4 & 256 & 0.9816 & 267.8s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t33 & 256 & 10 & 256 & 0.9815 & 1432.3s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t34 & 32 & 6 & 256 & 0.9809 & 403.2s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t35 & 32 & 4 & 512 & 0.9808 & 301.9s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t36 & 32 & 4 & 256 & 0.9808 & 266.2s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t37 & 64 & 4 & 512 & 0.9807 & 358.5s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t38 & 32 & 4 & 128 & 0.9798 & 266.7s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t39 & 32 & 4 & 64 & 0.9788 & 268.2s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\t40 & 32 & 6 & 512 & 0.9778 & 418.6s \\\\\n",
            "\t\t\t\\hline\n",
            "\t\t\\end{tabular}\n",
            "\t\\end{center}\n",
            "\t\\caption{Accuracy of different Transformer models on MNIST dataset}\n",
            "\t\\label{tab:transformer_accuracy}\n",
            "\\end{table}\n"
          ]
        }
      ],
      "source": [
        "import latextable\n",
        "from texttable import Texttable\n",
        "\n",
        "with open('results_kaggle.pkl', 'rb') as f:\n",
        "    new_values = pickle.load(f)\n",
        "    for key, value in new_values.items():\n",
        "        if key not in store:\n",
        "            store[key] = value\n",
        "\n",
        "table_1 = Texttable()\n",
        "table_1.set_cols_align([\"c\", \"c\", \"c\", \"c\", \"c\", \"c\"])\n",
        "table_1.set_precision(4)\n",
        "\n",
        "items = list(store.items())\n",
        "items.sort(key=lambda x: x[1]['accuracy'], reverse=True)\n",
        "rows = [[\n",
        "    \"\\\\textbf{Rank}\",\n",
        "    \"\\\\textbf{\\\\texttt{dim}}\",\n",
        "    \"\\\\textbf{\\\\texttt{depth}}\",\n",
        "    \"\\\\textbf{\\\\texttt{mlp\\\\_dim}}\",\n",
        "    \"\\\\textbf{\\\\texttt{accuracy}}\",\n",
        "    \"\\\\textbf{\\\\texttt{Exec Time}}\"\n",
        "]]\n",
        "for i, (key, value) in enumerate(items):\n",
        "    rows.append([i + 1, key[0], key[1], key[3], float(value['accuracy']), f\"{value['execution_time']:.1f}s\"])\n",
        "\n",
        "table_1.add_rows(rows)\n",
        "# print(table_1.draw())\n",
        "print(latextable.draw_latex(\n",
        "    table_1,\n",
        "    caption=\"Accuracy of different Transformer models on MNIST dataset\",\n",
        "    label=\"tab:transformer_accuracy\",\n",
        "    position=\"htbp\"\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot train loss of the best model\n",
        "best_model = items[0][1]\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(best_model['train_loss'], label='Train loss')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.savefig(FIGURE_PATH / 'ex3_3_train_loss.pdf')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
